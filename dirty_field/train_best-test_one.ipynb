{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/PycharmProjects/NodesEstimation/venv/lib/python3.9/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from abc import *\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import mne\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors._dist_metrics import DistanceMetric\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nodestimation.learning.estimation import collect_statistic, \\\n",
    "    compute_importance, collect_cross_statistic, make_selection_map, \\\n",
    "    select, separate_datasets, selected_statistic, choose_best, selected_data, make_feature_selection\n",
    "from nodestimation.learning.informativeness import CrossInformativeness, Informativeness, SubjectsInformativeness, \\\n",
    "    NodesInformativeness\n",
    "from nodestimation.learning.networking import sparse_graph, graph_to_hemispheres, hemispheres_division_modularity, \\\n",
    "    metric_for_hemispheres\n",
    "from nodestimation.processing.features import prepare_features\n",
    "from nodestimation.project import find_subject_dir, conditions_unique_code\n",
    "from nodestimation.pipeline import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nibabel\n",
    "import nilearn.plotting as nplt\n",
    "from nodestimation.project.actions import read\n",
    "import nodestimation as nd\n",
    "from nodestimation.learning.modification import append_series, promote\n",
    "import nodestimation.learning.modification as lmd\n",
    "from nodestimation.project.subject import Subject\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from nodestimation.learning.selection import SubjectsStatistic, Wilcoxon, Mannwhitneyu, Test\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.fftpack import fftfreq, irfft, rfft\n",
    "from scipy.fftpack import fftfreq, irfft, rfft\n",
    "\n",
    "ENGEL1 = [\n",
    "    'B1C2',\n",
    "    'B1R1',\n",
    "    'G1R1',\n",
    "    'G1V2',\n",
    "    'J1T2',\n",
    "    'K1V1',\n",
    "    'L1P1',\n",
    "    'M1G2',\n",
    "    'M1N2',\n",
    "    'O1O2',\n",
    "    'R1D2',\n",
    "    'S1A2',\n",
    "    'S1B1',\n",
    "    'S1H1',\n",
    "    'S1U3'\n",
    "]\n",
    "ENGEL2 = [\n",
    "    'L2M1',\n",
    "    'M2S2',\n",
    "    'N2K2',\n",
    "    'P1H2'\n",
    "]\n",
    "ENGEL34 = [\n",
    "    'N3S2',\n",
    "    'S3R1',\n",
    "    'K4L2'\n",
    "]\n",
    "REJECTED = [\n",
    "    'S1U3',\n",
    "    'P1H2'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All computation has been already done, loading of the existing file with the solution...\n"
     ]
    }
   ],
   "source": [
    "subjects = pipeline(\n",
    "    methods=['wpli', 'envelope', 'coh', 'imcoh', 'plv', 'ciplv', 'ppc', 'pli', 'pli2_unbiased', 'wpli2_debiased'],\n",
    "    freq_bands=(4, 8),\n",
    "    centrality_metrics=['eigen', 'between', 'degree', 'info']\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat done\n"
     ]
    }
   ],
   "source": [
    "stat = SubjectsStatistic(subjects, 'resected', centrality_metric='eigen')\n",
    "print('stat done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1H2: DONE, RUNTIME: 4.181505918502808\n",
      "M2S2: DONE, RUNTIME: 4.076422214508057\n",
      "R1D2: DONE, RUNTIME: 2.7417993545532227\n",
      "N3S2: DONE, RUNTIME: 1.8223857879638672\n",
      "S1A2: DONE, RUNTIME: 1.915980339050293\n",
      "S1H1: DONE, RUNTIME: 2.605848789215088\n",
      "K1V1: DONE, RUNTIME: 2.4667234420776367\n",
      "L1P1: DONE, RUNTIME: 1.923340082168579\n",
      "M1G2: DONE, RUNTIME: 2.8264050483703613\n",
      "G1V2: DONE, RUNTIME: 1.7512128353118896\n",
      "G1R1: DONE, RUNTIME: 1.7884962558746338\n",
      "M1N2: DONE, RUNTIME: 2.019461154937744\n",
      "S1B1: DONE, RUNTIME: 2.2403512001037598\n",
      "S1U3: DONE, RUNTIME: 1.8573274612426758\n",
      "B1R1: DONE, RUNTIME: 1.7702512741088867\n",
      "S3R1: DONE, RUNTIME: 2.151840925216675\n",
      "N2K2: DONE, RUNTIME: 3.0091543197631836\n",
      "K4L2: DONE, RUNTIME: 2.187037944793701\n",
      "B1C2: DONE, RUNTIME: 2.17913818359375\n",
      "J1T2: DONE, RUNTIME: 2.2158942222595215\n",
      "O1O2: DONE, RUNTIME: 2.1566057205200195\n",
      "L2M1: DONE, RUNTIME: 2.3954594135284424\n",
      "graph done\n"
     ]
    }
   ],
   "source": [
    "DATASETS = [\n",
    "    # metric_for_hemispheres(subjects, nx.algorithms.cluster.transitivity),\n",
    "    # metric_for_hemispheres(subjects, nx.algorithms.smetric.s_metric, normalized=False),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.global_efficiency)\n",
    "]\n",
    "print('graph done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "datasets = DATASETS.copy()\n",
    "names = ['global_efficiency']\n",
    "cross_hemispheres_informativeness_arr = list()\n",
    "cross_subjects_informativeness_arr = list()\n",
    "\n",
    "for dataset, name in zip(datasets, names):\n",
    "    cross_hemispheres_informativeness = CrossInformativeness()\n",
    "    cross_subjects_informativeness = CrossInformativeness()\n",
    "\n",
    "    for _ in range(100):\n",
    "        hemispheres_informatoveness = Informativeness()\n",
    "        subjects_informativeness = SubjectsInformativeness()\n",
    "        acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "        for i in range(100):\n",
    "\n",
    "            y = dataset['resected'].to_numpy()\n",
    "            x = dataset[[f'{name}_for_wpli_4-8Hz', f'{name}_for_envelope_4-8Hz']].to_numpy()\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            x = scaler.fit_transform(x)\n",
    "\n",
    "            samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "            x = np.append(x, samples, axis=1)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "            train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "            x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "            clf = svm.SVC()\n",
    "            clf.fit(x_train, y_train)\n",
    "            pred = clf.predict(x_test)\n",
    "\n",
    "            for predicted, actual, sample, value in zip(pred, y_test, test_samples, x_test):\n",
    "                hemispheres_informatoveness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "                subjects_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "\n",
    "        cross_subjects_informativeness.informativeness = subjects_informativeness\n",
    "        cross_hemispheres_informativeness.informativeness = hemispheres_informatoveness\n",
    "\n",
    "    cross_hemispheres_informativeness_arr.append(cross_hemispheres_informativeness)\n",
    "    cross_subjects_informativeness_arr.append(cross_subjects_informativeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cross_nodes_informativeness = CrossInformativeness()\n",
    "cross_subjects_informativeness = CrossInformativeness()\n",
    "cross_samples_informativeness = CrossInformativeness()\n",
    "\n",
    "for _ in range(100):\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "    samples_informativeness = Informativeness()\n",
    "    nodes_informativeness = NodesInformativeness()\n",
    "    subject_informativeness = SubjectsInformativeness()\n",
    "\n",
    "    for _ in range(100):\n",
    "        clf = svm.SVC()\n",
    "        true_data = stat.datasets['true'][features]\n",
    "        false_data = stat.datasets['false_mirror'][features]\n",
    "        true_data = true_data.assign(resected=True)\n",
    "        false_data = false_data.assign(resected=False)\n",
    "        dataset = pd.concat([true_data, false_data], axis=0)\n",
    "        dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        y = dataset['resected'].to_numpy()\n",
    "        dataset = dataset.drop(['resected'], axis=1)\n",
    "        samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "        x = scaler.fit_transform(dataset)\n",
    "        x = np.append(x, samples, axis=1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "        train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "        x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        pred = clf.predict(x_test)\n",
    "\n",
    "        for predicted, actual, sample, value in zip(pred, y_test, test_samples, x_test):\n",
    "            nodes_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "            subject_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "            samples_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "\n",
    "    cross_nodes_informativeness.informativeness = nodes_informativeness\n",
    "    cross_subjects_informativeness.informativeness = subject_informativeness\n",
    "    cross_samples_informativeness.informativeness = samples_informativeness\n",
    "\n",
    "cross_subjects_informativeness_arr.append(cross_subjects_informativeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# high (nodes), eigencentrality\n",
    "\n",
    "results = list()\n",
    "\n",
    "for percentage in [0.0, 0.6]:\n",
    "\n",
    "    test_subject = 'K1V1'\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    accur = cross_samples_informativeness.acc()\n",
    "\n",
    "    # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "    # clf = AdaBoostClassifier(n_estimators=10)\n",
    "    # clf = svm.SVC(kernel='sigmoid')\n",
    "    # clf = svm.SVC()\n",
    "    # clf = svm.SVC(kernel='linear')\n",
    "    # clf = SGDClassifier()\n",
    "    # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf = LogisticRegression(class_weight={True: 1, False: .7})\n",
    "    # clf = RandomForestClassifier(max_depth=20)\n",
    "    # clf = GaussianNB()\n",
    "    # clf = LinearDiscriminantAnalysis()\n",
    "    # clf = QuadraticDiscriminantAnalysis()\n",
    "    # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "    # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100, 100), max_iter=1450)\n",
    "\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if accur[sample] < 0.60 and test_subject not in sample:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if accur[sample] < percentage and test_subject not in sample:\n",
    "            false_data = false_data.drop(index=sample)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    for subject in subjects:\n",
    "        if subject.name == test_subject:\n",
    "            test_subject = subject\n",
    "            break\n",
    "\n",
    "    dataset = test_subject.datasets['eigen'][['4-8Hz_wpli', '4-8Hz_envelope', 'resected']]\n",
    "    # dataset = dataset.sample(frac = 1)\n",
    "\n",
    "    x_test = scaler.fit_transform(\n",
    "        dataset[['4-8Hz_wpli', '4-8Hz_envelope']].to_numpy()\n",
    "    )\n",
    "    y_test = dataset['resected'].to_numpy()\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'Training on 60%-above true data and {percentage*100}%-above false data, testing on {test_subject}')\n",
    "    print(f'acc: {(tn + tp)/(tn + tp + fn + fp)}')\n",
    "    print(f'spec: {tn / (tn + fp)}')\n",
    "    print(f'sens: {tp / (tp + fn)}\\n')\n",
    "\n",
    "    # print('test  pred')\n",
    "    # for test, pred in zip(y_test, y_pred):\n",
    "    #     print(test, pred)\n",
    "    results.append(y_pred)\n",
    "    if percentage == 0.6:\n",
    "        results.append(y_test)\n",
    "\n",
    "x1 = np.array([1 if res == True else 0 for res in results[0]])\n",
    "x2 = np.array([1 if res == True else 0 for res in results[1]])\n",
    "x = np.array([x1, x2])\n",
    "y = np.array(results[2])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "#\n",
    "# x = scaler.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.T, y.T)\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print('Training on previous results')\n",
    "print('acc: ', (tn + tp)/(tn + tp + fn + fp))\n",
    "print('spec: ', tn / (tn + fp))\n",
    "print('sens: ', tp / (tp + fn))\n",
    "\n",
    "\n",
    "# results = np.array(results)\n",
    "# t1 = np.array([True if res2 and not res1 else False for res1, res2 in zip(results[0], results[1])])\n",
    "# t2 = results[2]\n",
    "#\n",
    "# tn, fp, fn, tp = confusion_matrix(t2, t1).ravel()\n",
    "#\n",
    "# print('acc: ', (tn + tp)/(tn + tp + fn + fp))\n",
    "# print('spec: ', tn / (tn + fp))\n",
    "# print('sens: ', tp / (tp + fn))\n",
    "#\n",
    "# # for t11, t22 in zip(t1, t2):\n",
    "# #     print(t11, t22)\n",
    "#\n",
    "# # for t11, t22, res in zip(results[0], results[1], results[2]):\n",
    "# #     print(t22 and not t11, res)\n",
    "#\n",
    "# # for test, pred in zip(y_test, y_pred):\n",
    "# #     print(test, pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "percentage = 0.0\n",
    "test_subject = 'K1V1'\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "accur = cross_samples_informativeness.acc()\n",
    "\n",
    "clf = LogisticRegression(class_weight={True: 1, False: .6})\n",
    "# clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100, 100), max_iter=1450)\n",
    "\n",
    "true_data = stat.datasets['true'][features].copy()\n",
    "false_data = stat.datasets['false_mirror'][features].copy()\n",
    "true_data = true_data.assign(resected=True)\n",
    "false_data = false_data.assign(resected=False)\n",
    "for sample in true_data.index:\n",
    "    if accur[sample] < 0.80 and test_subject not in sample:\n",
    "        true_data = true_data.drop(index=sample)\n",
    "for sample in false_data.index:\n",
    "    if accur[sample] < percentage and test_subject not in sample:\n",
    "        false_data = false_data.drop(index=sample)\n",
    "\n",
    "dataset = pd.concat([true_data, false_data], axis=0)\n",
    "dataset = dataset.sample(frac = 1)\n",
    "# print(dataset)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y = dataset['resected'].to_numpy()\n",
    "dataset = dataset.drop(['resected'], axis=1)\n",
    "\n",
    "x = scaler.fit_transform(dataset)\n",
    "\n",
    "true, false = 0, 0\n",
    "\n",
    "for sample in y:\n",
    "    if sample:\n",
    "        true += 1\n",
    "    else:\n",
    "        false += 1\n",
    "\n",
    "print(true, false)\n",
    "\n",
    "clf.fit(x, y)\n",
    "\n",
    "for subject in subjects:\n",
    "    if subject.name == test_subject:\n",
    "        test_subject = subject\n",
    "        break\n",
    "\n",
    "dataset = test_subject.datasets['eigen'][['4-8Hz_wpli', '4-8Hz_envelope', 'resected']]\n",
    "dataset = dataset.sample(frac = 1)\n",
    "# print(dataset)\n",
    "\n",
    "x_test = scaler.fit_transform(\n",
    "    dataset[['4-8Hz_wpli', '4-8Hz_envelope']].to_numpy()\n",
    ")\n",
    "y_test = dataset['resected'].to_numpy()\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(f'Training on 60%-above true data and {percentage*100}%-above false data, testing on {test_subject}')\n",
    "print(f'acc: {(tn + tp)/(tn + tp + fn + fp)}')\n",
    "print(f'spec: {tn / (tn + fp)}')\n",
    "print(f'sens: {tp / (tp + fn)}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject S1H1 at ./Source/Subjects/S1H1 \n",
      "\n",
      "Subject K1V1 at ./Source/Subjects/K1V1 \n",
      "\n",
      "Subject M1G2 at ./Source/Subjects/M1G2 \n",
      "\n",
      "Subject G1R1 at ./Source/Subjects/G1R1 \n",
      "\n",
      "Subject M1N2 at ./Source/Subjects/M1N2 \n",
      "\n",
      "Subject N2K2 at ./Source/Subjects/N2K2 \n",
      "\n",
      "Subject B1C2 at ./Source/Subjects/B1C2 \n",
      "\n",
      "Subject J1T2 at ./Source/Subjects/J1T2 \n",
      "\n",
      "Subject O1O2 at ./Source/Subjects/O1O2 \n",
      "\n",
      "Subject L2M1 at ./Source/Subjects/L2M1 \n",
      "\n",
      "mean acc: 0.6525490196078432\n",
      "mean spec: 0.6524947217347054\n",
      "mean sens: 0.6431372549019608\n"
     ]
    }
   ],
   "source": [
    "accs = list()\n",
    "specs = list()\n",
    "senss = list()\n",
    "\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    if subject.name in ENGEL34 or subject.name in REJECTED:\n",
    "        continue\n",
    "\n",
    "    percentage = 0.10\n",
    "    test_subject = subject.name\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    accur = cross_samples_informativeness.acc()\n",
    "\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if accur[sample] < 0.80 or test_subject in sample:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if accur[sample] < 0.20 or test_subject in sample:\n",
    "            false_data = false_data.drop(index=sample)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    # print(dataset)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "\n",
    "    true, false = 0, 0\n",
    "\n",
    "    for sample in y:\n",
    "        if sample:\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "\n",
    "    # print(true, false, true/false)\n",
    "\n",
    "    # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "    # clf = AdaBoostClassifier(n_estimators=10)\n",
    "    # clf = svm.SVC(class_weight={True: 1, False: .5})\n",
    "    # clf = svm.SVC(class_weight={True: false/(true+false), False: true/(true+false)})\n",
    "    # clf = svm.SVC(kernel='sigmoid', class_weight={True: false/(true+false), False: true/(true+false)})\n",
    "    # clf = svm.SVC(kernel='poly', class_weight={True: false/(true+false), False: true/(true+false)})\n",
    "    # clf = svm.SVC(kernel='precomputed')\n",
    "    # clf = svm.SVC(kernel='linear', class_weight={True: false/(true+false), False: true/(true+false)})\n",
    "    # clf = SGDClassifier()\n",
    "    # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    # clf = LogisticRegression(class_weight={True: 1, False: .7})\n",
    "    # clf = LogisticRegression(class_weight={True: 1, False: 0.3})\n",
    "    # clf = LogisticRegression(class_weight={True: false/(true+false), False: true/(true+false)}) #*\n",
    "    # clf = RandomForestClassifier(max_depth=20)\n",
    "    # clf = GaussianNB()\n",
    "    clf = LinearDiscriminantAnalysis() #*\n",
    "    # clf = QuadraticDiscriminantAnalysis()\n",
    "    # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "    # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100, 100), max_iter=1450)\n",
    "\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    for subject in subjects:\n",
    "        if subject.name == test_subject:\n",
    "            test_subject = subject\n",
    "            break\n",
    "\n",
    "    dataset = test_subject.datasets['eigen'][['4-8Hz_wpli', '4-8Hz_envelope', 'resected']]\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    # print(dataset)\n",
    "\n",
    "    x_test = scaler.fit_transform(\n",
    "        dataset[['4-8Hz_wpli', '4-8Hz_envelope']].to_numpy()\n",
    "    )\n",
    "    y_test = dataset['resected'].to_numpy()\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accs.append((tn + tp)/(tn + tp + fn + fp))\n",
    "    specs.append(tn / (tn + fp))\n",
    "    senss.append(tp / (tp + fn))\n",
    "\n",
    "    if (tn + tp)/(tn + tp + fn + fp) > 0.5 and\\\n",
    "        (tn / (tn + fp)) > 0.5 and\\\n",
    "        (tp / (tp + fn)) > 0.5:\n",
    "        print(test_subject)\n",
    "\n",
    "    # print(f'Training on 60%-above true data and {percentage*100}%-above false data, testing on {test_subject}')\n",
    "    # print(f'acc: {(tn + tp)/(tn + tp + fn + fp)}')\n",
    "    # print(f'spec: {tn / (tn + fp)}')\n",
    "    # print(f'sens: {tp / (tp + fn)}\\n')\n",
    "\n",
    "print(f'mean acc: {np.array(accs).mean()}')\n",
    "print(f'mean spec: {np.array(specs).mean()}')\n",
    "print(f'mean sens: {np.array(senss).mean()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject M2S2 at ./Source/Subjects/M2S2 \n",
      "\n",
      "Subject R1D2 at ./Source/Subjects/R1D2 \n",
      "\n",
      "Subject S1H1 at ./Source/Subjects/S1H1 \n",
      "\n",
      "Subject M1G2 at ./Source/Subjects/M1G2 \n",
      "\n",
      "Subject G1V2 at ./Source/Subjects/G1V2 \n",
      "\n",
      "Subject G1R1 at ./Source/Subjects/G1R1 \n",
      "\n",
      "Subject O1O2 at ./Source/Subjects/O1O2 \n",
      "\n",
      "Subject L2M1 at ./Source/Subjects/L2M1 \n",
      "\n",
      "mean acc: 0.6862745098039216\n",
      "mean spec: 0.6558823529411765\n",
      "mean sens: 0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "accs = list()\n",
    "specs = list()\n",
    "senss = list()\n",
    "\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    if subject.name in ENGEL34 or subject.name in REJECTED:\n",
    "        continue\n",
    "\n",
    "    percentage = 0.10\n",
    "    test_subject = subject.name\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    accur = cross_samples_informativeness.acc()\n",
    "\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if accur[sample] < 0.80 or test_subject in sample:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if accur[sample] < 0.20 or test_subject in sample:\n",
    "            false_data = false_data.drop(index=sample)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    # print(dataset)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "\n",
    "    true, false = 0, 0\n",
    "\n",
    "    for sample in y:\n",
    "        if sample:\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "\n",
    "    # print(true, false, true/false)\n",
    "\n",
    "    # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "    # clf = AdaBoostClassifier(n_estimators=10)\n",
    "    # clf = svm.SVC(class_weight={True: 1, False: .5})\n",
    "    # clf = svm.SVC(class_weight={True: false/(true+false), False: true/(true+false)})\n",
    "    # clf = svm.SVC(kernel='sigmoid', class_weight={True: false/(true+false), False: true/(true+false)}) #**\n",
    "    # clf = svm.SVC(kernel='poly', class_weight={True: false/(true+false), False: true/(true+false)})\n",
    "    # clf = svm.SVC(kernel='precomputed')\n",
    "    clf = svm.SVC(kernel='linear', class_weight={True: false/(true+false), False: true/(true+false)}) #**\n",
    "    # clf = SGDClassifier()\n",
    "    # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    # clf = LogisticRegression(class_weight={True: 1, False: .7}) #**\n",
    "    # clf = LogisticRegression(class_weight={True: 1, False: 0.3})\n",
    "    # clf = LogisticRegression(class_weight={True: false/(true+false), False: true/(true+false)}) #*\n",
    "    # clf = RandomForestClassifier(max_depth=20)\n",
    "    # clf = GaussianNB()\n",
    "    # clf = LinearDiscriminantAnalysis()\n",
    "    # clf = QuadraticDiscriminantAnalysis()\n",
    "    # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "    # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100, 100), max_iter=1450)\n",
    "\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    for subject in subjects:\n",
    "        if subject.name == test_subject:\n",
    "            test_subject = subject\n",
    "            break\n",
    "\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if test_subject.name not in sample:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if test_subject.name not in sample:\n",
    "            false_data = false_data.drop(index=sample)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    # print(dataset)\n",
    "\n",
    "    x_test = scaler.fit_transform(\n",
    "        dataset[['4-8Hz_wpli', '4-8Hz_envelope']].to_numpy()\n",
    "    )\n",
    "    y_test = dataset['resected'].to_numpy()\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accs.append((tn + tp)/(tn + tp + fn + fp))\n",
    "    specs.append(tn / (tn + fp))\n",
    "    senss.append(tp / (tp + fn))\n",
    "\n",
    "    if (tn + tp)/(tn + tp + fn + fp) > 0.5 and\\\n",
    "        (tn / (tn + fp)) > 0.5 and\\\n",
    "        (tp / (tp + fn)) > 0.5:\n",
    "        print(test_subject)\n",
    "\n",
    "    # print(f'Training on 60%-above true data and {percentage*100}%-above false data, testing on {test_subject}')\n",
    "    # print(f'acc: {(tn + tp)/(tn + tp + fn + fp)}')\n",
    "    # print(f'spec: {tn / (tn + fp)}')\n",
    "    # print(f'sens: {tp / (tp + fn)}\\n')\n",
    "\n",
    "print(f'mean acc: {np.array(accs).mean()}')\n",
    "print(f'mean spec: {np.array(specs).mean()}')\n",
    "print(f'mean sens: {np.array(senss).mean()}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = list()\n",
    "\n",
    "for clf, i in zip([LogisticRegression(class_weight={True: 1, False: .7}),\n",
    "            LogisticRegression(class_weight={True: 1, False: .3})], range(2)):\n",
    "\n",
    "    test_subject = 'K1V1'\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    accur = cross_samples_informativeness.acc()\n",
    "\n",
    "    # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "    # clf = AdaBoostClassifier(n_estimators=10)\n",
    "    # clf = svm.SVC(kernel='sigmoid')\n",
    "    # clf = svm.SVC()\n",
    "    # clf = svm.SVC(kernel='linear')\n",
    "    # clf = SGDClassifier()\n",
    "    # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    # clf = RandomForestClassifier(max_depth=20)\n",
    "    # clf = GaussianNB()\n",
    "    # clf = LinearDiscriminantAnalysis()\n",
    "    # clf = QuadraticDiscriminantAnalysis()\n",
    "    # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "    # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100, 100), max_iter=1450)\n",
    "\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if accur[sample] < 0.80 and test_subject not in sample:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    # for sample in false_data.index:\n",
    "    #     if accur[sample] < percentage and test_subject not in sample:\n",
    "    #         false_data = false_data.drop(index=sample)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    for subject in subjects:\n",
    "        if subject.name == test_subject:\n",
    "            test_subject = subject\n",
    "            break\n",
    "\n",
    "    dataset = test_subject.datasets['eigen'][['4-8Hz_wpli', '4-8Hz_envelope', 'resected']]\n",
    "    # dataset = dataset.sample(frac = 1)\n",
    "\n",
    "    x_test = scaler.fit_transform(\n",
    "        dataset[['4-8Hz_wpli', '4-8Hz_envelope']].to_numpy()\n",
    "    )\n",
    "    y_test = dataset['resected'].to_numpy()\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    print(f'Training on 60%-above true data and {percentage*100}%-above false data, testing on {test_subject}')\n",
    "    print(f'acc: {(tn + tp)/(tn + tp + fn + fp)}')\n",
    "    print(f'spec: {tn / (tn + fp)}')\n",
    "    print(f'sens: {tp / (tp + fn)}\\n')\n",
    "\n",
    "    # print('test  pred')\n",
    "    # for test, pred in zip(y_test, y_pred):\n",
    "    #     print(test, pred)\n",
    "    results.append(y_pred)\n",
    "    if i == 1:\n",
    "        results.append(y_test)\n",
    "\n",
    "x1 = np.array([1 if res == True else 0 for res in results[0]])\n",
    "x2 = np.array([1 if res == True else 0 for res in results[1]])\n",
    "x = np.array([x1, x2])\n",
    "y = np.array(results[2])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "#\n",
    "# x = scaler.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.T, y.T)\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print('Training on previous results')\n",
    "print('acc: ', (tn + tp)/(tn + tp + fn + fp))\n",
    "print('spec: ', tn / (tn + fp))\n",
    "print('sens: ', tp / (tp + fn))\n",
    "\n",
    "\n",
    "# results = np.array(results)\n",
    "# t1 = np.array([True if res2 and not res1 else False for res1, res2 in zip(results[0], results[1])])\n",
    "# t2 = results[2]\n",
    "#\n",
    "# tn, fp, fn, tp = confusion_matrix(t2, t1).ravel()\n",
    "#\n",
    "# print('acc: ', (tn + tp)/(tn + tp + fn + fp))\n",
    "# print('spec: ', tn / (tn + fp))\n",
    "# print('sens: ', tp / (tp + fn))\n",
    "#\n",
    "# # for t11, t22 in zip(t1, t2):\n",
    "# #     print(t11, t22)\n",
    "#\n",
    "score1, score2 = list(), list()\n",
    "for t22, t11, res in zip(results[0], results[1], results[2]):\n",
    "    if t11:\n",
    "        score1.append(t11)\n",
    "        print(t11, res, t22, 't22')\n",
    "    elif not t22:\n",
    "        score1.append(t22)\n",
    "        print(t22, res, t11, 't11')\n",
    "    else:\n",
    "        print('None', res)\n",
    "#     if not t22:\n",
    "#         score1.append(t22)\n",
    "#         # print(t22, res)\n",
    "#     elif t11:\n",
    "#         score2.append(t11)\n",
    "#         # print(t11, res)\n",
    "#\n",
    "# tn, fp, fn, tp = confusion_matrix(np.array(score1), results[2]).ravel()\n",
    "#\n",
    "# print('Training on previous results')\n",
    "# print('acc: ', (tn + tp)/(tn + tp + fn + fp))\n",
    "# print('spec: ', tn / (tn + fp))\n",
    "# print('sens: ', tp / (tp + fn))\n",
    "#\n",
    "# tn, fp, fn, tp = confusion_matrix(score2, results[2]).ravel()\n",
    "#\n",
    "# print('Training on previous results')\n",
    "# print('acc: ', (tn + tp)/(tn + tp + fn + fp))\n",
    "# print('spec: ', tn / (tn + fp))\n",
    "# print('sens: ', tp / (tp + fn))\n",
    "\n",
    "#\n",
    "# # for test, pred in zip(y_test, y_pred):\n",
    "# #     print(test, pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}