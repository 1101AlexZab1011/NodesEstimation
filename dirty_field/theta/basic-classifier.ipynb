{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/nilearn/datasets/__init__.py:86: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from abc import *\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "\n",
    "from networkx.algorithms.approximation import large_clique_size\n",
    "\n",
    "from nodestimation.learning.connectoming import *\n",
    "from nodestimation.learning.modification import normalize_df\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import mne\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors._dist_metrics import DistanceMetric\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nodestimation.learning.estimation import collect_statistic, \\\n",
    "    compute_importance, collect_cross_statistic, make_selection_map, \\\n",
    "    select, separate_datasets, selected_statistic, choose_best, selected_data, make_feature_selection\n",
    "from nodestimation.learning.informativeness import CrossInformativeness, Informativeness, SubjectsInformativeness, \\\n",
    "    NodesInformativeness\n",
    "from nodestimation.learning.networking import sparse_graph, graph_to_hemispheres, hemispheres_division_modularity, \\\n",
    "    metric_for_hemispheres\n",
    "from nodestimation.processing.features import prepare_features\n",
    "from nodestimation.project import find_subject_dir, conditions_unique_code\n",
    "from nodestimation.pipeline import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nibabel\n",
    "import nilearn.plotting as nplt\n",
    "from nodestimation.project.actions import read\n",
    "import nodestimation as nd\n",
    "from nodestimation.learning.modification import append_series, promote\n",
    "import nodestimation.learning.modification as lmd\n",
    "from nodestimation.project.subject import Subject\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib as mpl\n",
    "from nodestimation.learning.selection import SubjectsStatistic, Wilcoxon, Mannwhitneyu, Test\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.fftpack import fftfreq, irfft, rfft\n",
    "from scipy.fftpack import fftfreq, irfft, rfft"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SUBJECTS = pipeline(\n",
    "    methods=['wpli', 'envelope', 'coh', 'imcoh', 'plv', 'ciplv', 'ppc', 'pli', 'pli2_unbiased', 'wpli2_debiased'],\n",
    "    freq_bands=(4, 8),\n",
    "    centrality_metrics=['eigen', 'between', 'degree', 'info']\n",
    "    )\n",
    "\n",
    "ENGEL1 = [\n",
    "    'B1C2',\n",
    "    'B1R1',\n",
    "    'G1R1',\n",
    "    'G1V2',\n",
    "    'J1T2',\n",
    "    'K1V1',\n",
    "    'L1P1',\n",
    "    'M1G2',\n",
    "    'M1N2',\n",
    "    'O1O2',\n",
    "    'R1D2',\n",
    "    'S1A2',\n",
    "    'S1B1',\n",
    "    'S1H1',\n",
    "    'S1U3'\n",
    "]\n",
    "ENGEL2 = [\n",
    "    'L2M1',\n",
    "    'M2S2',\n",
    "    'N2K2',\n",
    "    'P1H2'\n",
    "]\n",
    "ENGEL34 = [\n",
    "    'N3S2',\n",
    "    'S3R1',\n",
    "    'K4L2'\n",
    "]\n",
    "REJECTED = [\n",
    "    'S1U3',\n",
    "    'P1H2'\n",
    "]\n",
    "\n",
    "INCLUDED = [\n",
    "    'B1R1',\n",
    "    'G1R1',\n",
    "    'G1V2',\n",
    "    'L1P1',\n",
    "    'M1G2',\n",
    "    'M1N2',\n",
    "    'O1O2',\n",
    "    'R1D2',\n",
    "]\n",
    "\n",
    "# subjects = SUBJECTS.copy()\n",
    "subjects = [\n",
    "        subject\n",
    "        for subject in SUBJECTS\n",
    "        if subject.name not in REJECTED\n",
    "           # and subject.name not in ENGEL34\n",
    "    ]\n",
    "\n",
    "STAT = SubjectsStatistic(\n",
    "    subjects,\n",
    "    'resected',\n",
    "    centrality_metric='eigen'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GRAPHS = [\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.cluster.transitivity),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.smetric.s_metric, normalized=False),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.global_efficiency),\n",
    "    # additional metrics\n",
    "    # metric: accuracy | specificity | sensitivity with Engel 1&2 groups\n",
    "    # metric_for_hemispheres(subjects, nx.algorithms.cluster.average_clustering, weight='weight'), # 51 | 52 | 57\n",
    "    # metric_for_hemispheres(subjects, large_clique_size), # 62 | 68 | 60\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 75% of engel 1 and 2 -> train set, 25% engel 1 and 2 -> test set, the rest -> test set\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "for dataset, name in zip(GRAPHS, ['transitivity', 's_metric', 'global_efficiency']):\n",
    "# for dataset, name in zip(GRAPHS, ['average_clustering']):\n",
    "    for _ in range(1000):\n",
    "\n",
    "        y = dataset['resected'].to_numpy()\n",
    "        index = dataset.index\n",
    "        x = dataset[[\n",
    "            f'{name}_for_wpli_4-8Hz',\n",
    "            f'{name}_for_envelope_4-8Hz'\n",
    "        ]].to_numpy()\n",
    "        scaler = StandardScaler()\n",
    "        x = scaler.fit_transform(x)\n",
    "        x_train, y_train = list(), list()\n",
    "        x_test, y_test = list(), list()\n",
    "        for i, sample, sample_name in zip(index, x, y):\n",
    "            if any([subject in i for subject in ENGEL1]):\n",
    "                    x_train.append(sample)\n",
    "                    y_train.append(sample_name)\n",
    "            else:\n",
    "                x_test.append(sample)\n",
    "                y_test.append(sample_name)\n",
    "\n",
    "        x_train, x_test1, y_train, y_test1 = train_test_split(x_train, y_train, train_size=0.5)\n",
    "\n",
    "        x_train = np.array(x_train)\n",
    "        x_test = np.array([*x_test, *x_test1])\n",
    "        y_test = np.array([*y_test, *y_test1])\n",
    "\n",
    "        # x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "        # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "        # clf = AdaBoostClassifier(n_estimators=10)\n",
    "        # clf = svm.SVC(kernel='sigmoid')\n",
    "        clf = svm.SVC()\n",
    "        # clf = svm.SVC(kernel='linear')\n",
    "        # clf = svm.SVC(kernel='poly')\n",
    "        # clf = SGDClassifier()\n",
    "        # clf = KNeighborsClassifier(n_neighbors=7, metric='chebyshev')\n",
    "        # clf = LogisticRegression(class_weight={True: 1, False: .8})\n",
    "        # clf = LogisticRegression()\n",
    "        # clf = RandomForestClassifier(max_depth=20)\n",
    "        # clf = GaussianNB()\n",
    "        # clf = LinearDiscriminantAnalysis()\n",
    "        # clf = QuadraticDiscriminantAnalysis()\n",
    "        # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "        # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(10, 10), max_iter=1450)\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        pred = clf.predict(x_test)\n",
    "\n",
    "        acc.append(accuracy_score(y_test, pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0,1]).ravel()\n",
    "        if not any([tn + fp == 0, tp + fn == 0, tn + fn == 0, tp + fp == 0]):\n",
    "            spec.append(tn / (tn + fp))\n",
    "            sens.append(tp / (tp + fn))\n",
    "            negpred.append(tn/(tn + fn))\n",
    "            pospred.append(tp/(tp + fp))\n",
    "    spec = [s for s in spec if not np.isnan(s)]\n",
    "    sens = [s for s in sens if not np.isnan(s)]\n",
    "    print(name)\n",
    "    print('acc: ', np.array(acc).mean())\n",
    "    print('spec: ', np.array(spec).mean())\n",
    "    print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "\n",
    "# features = ['4-8Hz_plv', '4-8Hz_envelope']\n",
    "#\n",
    "#\n",
    "# true_data = stat.datasets['true'][features]\n",
    "# false_data = stat.datasets['false_mirror'][features]\n",
    "#\n",
    "# engel_1_true = true_data.loc[(elem[:4] in ENGEL1 for elem in true_data.index)]\n",
    "# engel_1_false = false_data.loc[(elem[:4] in ENGEL1 for elem in false_data.index)]\n",
    "#\n",
    "# engel_2_3_4_true = true_data.loc[(elem[:4] not in ENGEL1 for elem in true_data.index)]\n",
    "# engel_2_3_4_false = false_data.loc[(elem[:4] not in ENGEL1 for elem in false_data.index)]\n",
    "#\n",
    "# engel_1_2_true = true_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] for elem in true_data.index)]\n",
    "# engel_1_2_false = false_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] for elem in false_data.index)]\n",
    "#\n",
    "# engel_3_4_true = true_data.loc[(elem[:4] not in [*ENGEL1, *ENGEL2] for elem in true_data.index)]\n",
    "# engel_3_4_false = false_data.loc[(elem[:4] not in [*ENGEL1, *ENGEL2] for elem in false_data.index)]\n",
    "#\n",
    "# engel_1 = pd.concat([engel_1_true, engel_1_false], axis=0)\n",
    "#\n",
    "# d1, d2 = np.array_split(engel_1.sample(frac=1), 2)\n",
    "#\n",
    "# d1\n",
    "\n",
    "path = f'/home/user//Documents/initial_stats.pkl'\n",
    "stats = pickle.load(open(path, 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eigen_stat = stats[0]\n",
    "perloc_stat = stats[-1]\n",
    "\n",
    "full_true_data_eigen = eigen_stat.datasets['true']['4-8Hz_wpli']\n",
    "full_true_data_perloc = perloc_stat.datasets['true']['4-8Hz_envelope']\n",
    "full_false_data_eigen = eigen_stat.datasets['false_mirror']['4-8Hz_wpli']\n",
    "full_false_data_perloc = perloc_stat.datasets['false_mirror']['4-8Hz_envelope']\n",
    "\n",
    "full_true_data = pd.concat([full_true_data_eigen, full_true_data_perloc], axis=1)\n",
    "\n",
    "full_false_data = pd.concat([full_false_data_eigen, full_false_data_perloc], axis=1)\n",
    "\n",
    "\n",
    "features = ['4-8Hz_envelope', '4-8Hz_wpli']\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "stat = deepcopy(STAT)\n",
    "\n",
    "for i in range(1000):\n",
    "    # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "    # clf = AdaBoostClassifier(n_estimators=10)\n",
    "    clf = svm.SVC(kernel='sigmoid')\n",
    "    # clf = svm.SVC()\n",
    "    # clf = svm.SVC(kernel='linear')\n",
    "    # clf = svm.SVC(kernel='poly')\n",
    "    # clf = SGDClassifier()\n",
    "    # clf = KNeighborsClassifier(n_neighbors=7, metric='chebyshev')\n",
    "    # clf = LogisticRegression(class_weight={True: 1, False: .8})\n",
    "    # clf = LogisticRegression()\n",
    "    # clf = RandomForestClassifier(max_depth=20)\n",
    "    # clf = GaussianNB()\n",
    "    # clf = LinearDiscriminantAnalysis()\n",
    "    # clf = QuadraticDiscriminantAnalysis()\n",
    "    # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "    # clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(10, 10), max_iter=1450)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # full_true_data = stat.datasets['true'][features]\n",
    "    # full_false_data = stat.datasets['false_mirror'][features]\n",
    "\n",
    "    ## All data\n",
    "    # true_data = full_true_data.assign(resected=True)\n",
    "    # false_data = full_false_data.assign(resected=False)\n",
    "\n",
    "    ## Engel1 only\n",
    "    # engel_1_true = full_true_data.loc[(elem[:4] in ENGEL1 for elem in full_true_data.index)]\n",
    "    # engel_1_false = full_false_data.loc[(elem[:4] in ENGEL1 for elem in full_false_data.index)]\n",
    "    # true_data = engel_1_true.assign(resected=True)\n",
    "    # false_data = engel_1_false.assign(resected=False)\n",
    "\n",
    "    ## Engel1&2 data\n",
    "    engel_1_2_true = full_true_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_true_data.index)]\n",
    "    engel_1_2_false = full_false_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_false_data.index)]\n",
    "    true_data = engel_1_2_true.assign(resected=True)\n",
    "    false_data = engel_1_2_false.assign(resected=False)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    ## Engel1 - train, the rest - test\n",
    "    # engel_1_true = full_true_data.loc[(elem[:4] in ENGEL1 and not elem[:4] in REJECTED for elem in full_true_data.index)].assign(resected=True)\n",
    "    # engel_1_false = full_false_data.loc[(elem[:4] in ENGEL1 and not elem[:4] in REJECTED for elem in full_false_data.index)].assign(resected=False)\n",
    "    # engel_2_3_4_true = full_true_data\\\n",
    "    #     .loc[(elem[:4] not in ENGEL1 and not elem[:4] in REJECTED for elem in full_true_data.index)].assign(resected=True)\n",
    "    # engel_2_3_4_false = full_false_data\\\n",
    "    #     .loc[(elem[:4] not in ENGEL1 and not elem[:4] in REJECTED for elem in full_false_data.index)].assign(resected=False)\n",
    "    # engel_1 = pd.concat([engel_1_true, engel_1_false], axis=0)\n",
    "    # engel_1_train, engel_1_test = np.array_split(engel_1.sample(frac=1), 2)\n",
    "    # engel_2_3_4 = pd.concat([engel_2_3_4_true, engel_2_3_4_false], axis=0)\n",
    "    #\n",
    "    # y_train = engel_1_train['resected'].to_numpy()\n",
    "    # x_train = scaler.fit_transform(engel_1_train.drop(['resected'], axis=1))\n",
    "    #\n",
    "    # engel_test = pd.concat([engel_2_3_4, engel_1_test], axis=0).sample(frac=1)\n",
    "    #\n",
    "    # y_test = engel_test['resected'].to_numpy()\n",
    "    # x_test = scaler.fit_transform(engel_test.drop(['resected'], axis=1))\n",
    "\n",
    "\n",
    "    # # Engel1&2 - train, the rest - test\n",
    "    # engel_1_2_true = full_true_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_true_data.index)].assign(resected=True)\n",
    "    # engel_1_2_false = full_false_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_false_data.index)].assign(resected=False)\n",
    "    # engel_3_4_true = full_true_data\\\n",
    "    #     .loc[(elem[:4] not in [*ENGEL1, *ENGEL2, *REJECTED] for elem in full_true_data.index)].assign(resected=True)\n",
    "    # engel_3_4_false = full_false_data\\\n",
    "    #     .loc[(elem[:4] not in [*ENGEL1, *ENGEL2, *REJECTED] for elem in full_false_data.index)].assign(resected=False)\n",
    "    # engel_1_2 = pd.concat([engel_1_2_true, engel_1_2_false], axis=0)\n",
    "    # engel_1_2_train, engel_1_2_test = np.array_split(engel_1_2.sample(frac=1), 2)\n",
    "    # engel_3_4 = pd.concat([engel_3_4_true, engel_3_4_false], axis=0)\n",
    "    #\n",
    "    # y_train = engel_1_2_train['resected'].to_numpy()\n",
    "    # x_train = scaler.fit_transform(engel_1_2_train.drop(['resected'], axis=1))\n",
    "    #\n",
    "    # engel_test = pd.concat([engel_3_4, engel_1_2_test], axis=0).sample(frac=1)\n",
    "    #\n",
    "    # y_test = engel_test['resected'].to_numpy()\n",
    "    # x_test = scaler.fit_transform(engel_test.drop(['resected'], axis=1))\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0,1]).ravel()\n",
    "    if not any([tn + fp == 0, tp + fn == 0, tn + fn == 0, tp + fp == 0]):\n",
    "        spec.append(tn / (tn + fp))\n",
    "        sens.append(tp / (tp + fn))\n",
    "        negpred.append(tn/(tn + fn))\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(stats[-2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## check all possible combinations of features (nodes)\n",
    "\n",
    "\n",
    "index = [stat.centrality_metric for stat in stats]\n",
    "\n",
    "for feat in ['wpli', 'coh', 'imcoh', 'plv', 'ciplv', 'ppc', 'pli', 'pli2_unbiased', 'wpli2_debiased']:\n",
    "    series = list()\n",
    "    for stat1 in stats:\n",
    "        curr_series = list()\n",
    "        for stat2 in stats:\n",
    "            full_true_data_1 = stat1.datasets['true'][f'4-8Hz_{feat}']\n",
    "            full_true_data_2 = stat2.datasets['true']['4-8Hz_envelope']\n",
    "            full_false_data_1 = stat1.datasets['false_mirror'][f'4-8Hz_{feat}']\n",
    "            full_false_data_2 = stat2.datasets['false_mirror']['4-8Hz_envelope']\n",
    "\n",
    "            full_true_data = pd.concat([full_true_data_1, full_true_data_2], axis=1)\n",
    "\n",
    "            full_false_data = pd.concat([full_false_data_1, full_false_data_2], axis=1)\n",
    "\n",
    "\n",
    "            features = ['4-8Hz_envelope', f'4-8Hz_{feat}']\n",
    "\n",
    "            acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "            for i in range(1000):\n",
    "                clf = svm.SVC(kernel='sigmoid')\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "                ## Engel1&2 data\n",
    "                engel_1_2_true = full_true_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_true_data.index)]\n",
    "                engel_1_2_false = full_false_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_false_data.index)]\n",
    "                true_data = engel_1_2_true.assign(resected=True)\n",
    "                false_data = engel_1_2_false.assign(resected=False)\n",
    "\n",
    "                dataset = pd.concat([true_data, false_data], axis=0)\n",
    "                dataset = dataset.sample(frac = 1)\n",
    "\n",
    "                y = dataset['resected'].to_numpy()\n",
    "                dataset = dataset.drop(['resected'], axis=1)\n",
    "                x = scaler.fit_transform(dataset)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "                clf.fit(x_train, y_train)\n",
    "                pred = clf.predict(x_test)\n",
    "\n",
    "                acc.append(accuracy_score(y_test, pred))\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0,1]).ravel()\n",
    "                if not any([tn + fp == 0, tp + fn == 0, tn + fn == 0, tp + fp == 0]):\n",
    "                    spec.append(tn / (tn + fp))\n",
    "                    sens.append(tp / (tp + fn))\n",
    "                    negpred.append(tn/(tn + fn))\n",
    "                    pospred.append(tp/(tp + fp))\n",
    "\n",
    "            curr_series.append(np.array(acc).mean())\n",
    "            # print(f'{stat1.centrality_metric} for {features[1]},\\n{stat2.centrality_metric} for {features[0]}')\n",
    "            # print('acc: ', np.array(acc).mean())\n",
    "            # print('spec: ', np.array(spec).mean())\n",
    "            # print('sens: ', np.array(sens).mean())\n",
    "        series.append(pd.Series(curr_series))\n",
    "\n",
    "    df = pd.DataFrame(series, index=index).rename(columns={i: col for i, col in enumerate(index)})\n",
    "    df.to_csv(f'/home/user/Documents/metrics_accuracy_{feat}.csv')\n",
    "    print(f'{features[1]}|{features[0]}')\n",
    "    print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "subjects = [subject for subject in SUBJECTS if subject.name in [*ENGEL1, *ENGEL2]]\n",
    "GRAPHS = [\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.cluster.transitivity),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.smetric.s_metric, normalized=False),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.global_efficiency),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.cluster.average_clustering, weight='weight'),\n",
    "\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "# print(GRAPHS[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitivity_for_wpli_4-8Hz,\n",
      "transitivity_for_envelope_4-8Hz\n",
      "acc:  0.7334444444444445\n",
      "spec:  0.7852042518709186\n",
      "sens:  0.7120656370656371\n",
      "transitivity_for_wpli_4-8Hz,\n",
      "s_metric_for_envelope_4-8Hz\n",
      "acc:  0.7592222222222221\n",
      "spec:  0.7815601566604576\n",
      "sens:  0.7581470602283039\n",
      "transitivity_for_wpli_4-8Hz,\n",
      "global_efficiency_for_envelope_4-8Hz\n",
      "acc:  0.759\n",
      "spec:  0.7878187711521045\n",
      "sens:  0.7568854568854568\n",
      "transitivity_for_wpli_4-8Hz,\n",
      "average_clustering_for_envelope_4-8Hz\n",
      "acc:  0.6862222222222223\n",
      "spec:  0.7661181373958033\n",
      "sens:  0.6344327871993868\n",
      "s_metric_for_wpli_4-8Hz,\n",
      "transitivity_for_envelope_4-8Hz\n",
      "acc:  0.7348888888888889\n",
      "spec:  0.806215601818617\n",
      "sens:  0.6897858339315627\n",
      "s_metric_for_wpli_4-8Hz,\n",
      "s_metric_for_envelope_4-8Hz\n",
      "acc:  0.7515555555555556\n",
      "spec:  0.7938028370826766\n",
      "sens:  0.728167836843865\n",
      "s_metric_for_wpli_4-8Hz,\n",
      "global_efficiency_for_envelope_4-8Hz\n",
      "acc:  0.7507777777777779\n",
      "spec:  0.7902660362038497\n",
      "sens:  0.7411806849118784\n",
      "s_metric_for_wpli_4-8Hz,\n",
      "average_clustering_for_envelope_4-8Hz\n",
      "acc:  0.6828888888888889\n",
      "spec:  0.8146499734645631\n",
      "sens:  0.5903097409176438\n",
      "global_efficiency_for_wpli_4-8Hz,\n",
      "transitivity_for_envelope_4-8Hz\n",
      "acc:  0.7313333333333335\n",
      "spec:  0.7612569324918722\n",
      "sens:  0.7378537961369287\n",
      "global_efficiency_for_wpli_4-8Hz,\n",
      "s_metric_for_envelope_4-8Hz\n",
      "acc:  0.7384444444444445\n",
      "spec:  0.7871490538157204\n",
      "sens:  0.7126114209447542\n",
      "global_efficiency_for_wpli_4-8Hz,\n",
      "global_efficiency_for_envelope_4-8Hz\n",
      "acc:  0.7564444444444445\n",
      "spec:  0.8068906578695736\n",
      "sens:  0.7338329508510232\n",
      "global_efficiency_for_wpli_4-8Hz,\n",
      "average_clustering_for_envelope_4-8Hz\n",
      "acc:  0.6924444444444443\n",
      "spec:  0.7957703927492448\n",
      "sens:  0.6320457488131204\n",
      "average_clustering_for_wpli_4-8Hz,\n",
      "transitivity_for_envelope_4-8Hz\n",
      "acc:  0.6662222222222222\n",
      "spec:  0.7148203375036369\n",
      "sens:  0.6782538066142955\n",
      "average_clustering_for_wpli_4-8Hz,\n",
      "s_metric_for_envelope_4-8Hz\n",
      "acc:  0.7081111111111111\n",
      "spec:  0.6785331803404093\n",
      "sens:  0.768864744693058\n",
      "average_clustering_for_wpli_4-8Hz,\n",
      "global_efficiency_for_envelope_4-8Hz\n",
      "acc:  0.7416666666666667\n",
      "spec:  0.709189661780205\n",
      "sens:  0.8131838650953339\n",
      "average_clustering_for_wpli_4-8Hz,\n",
      "average_clustering_for_envelope_4-8Hz\n",
      "acc:  0.5081111111111111\n",
      "spec:  0.6547095340282035\n",
      "sens:  0.43002120376047415\n",
      "average_clustering_for_wpli_4-8Hz|average_clustering_for_envelope_4-8Hz\n",
      "                    transitivity  s_metric  global_efficiency  \\\n",
      "transitivity            0.733444  0.759222           0.759000   \n",
      "s_metric                0.734889  0.751556           0.750778   \n",
      "global_efficiency       0.731333  0.738444           0.756444   \n",
      "average_clustering      0.666222  0.708111           0.741667   \n",
      "\n",
      "                    average_clustering  \n",
      "transitivity                  0.686222  \n",
      "s_metric                      0.682889  \n",
      "global_efficiency             0.692444  \n",
      "average_clustering            0.508111  \n"
     ]
    }
   ],
   "source": [
    "## check all possible combinations of features (hemispheres)\n",
    "index = ['transitivity', 's_metric', 'global_efficiency', 'average_clustering']\n",
    "features = ['wpli', 'coh', 'imcoh', 'plv', 'ciplv', 'ppc', 'pli', 'pli2_unbiased', 'wpli2_debiased']\n",
    "\n",
    "\n",
    "for feat in [features[0]]:\n",
    "    series = list()\n",
    "    for graph1, kind1 in zip(GRAPHS, index):\n",
    "        curr_series = list()\n",
    "        for graph2, kind2 in zip(GRAPHS, index):\n",
    "            assert all(graph1['resected'] == graph2['resected'])\n",
    "            full_data_1 = graph1[[f'{kind1}_for_{feat}_4-8Hz', 'resected']]\n",
    "            full_data_2 = graph2[[f'{kind2}_for_envelope_4-8Hz']]\n",
    "            full_data = pd.concat([full_data_1, full_data_2], axis=1)\n",
    "            features = [f'{kind2}_for_envelope_4-8Hz', f'{kind1}_for_{feat}_4-8Hz']\n",
    "\n",
    "            acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "            for i in range(1000):\n",
    "                clf = svm.SVC(kernel='sigmoid')\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "                ## Engel1&2 data\n",
    "                engel_1_2 = full_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_data.index)]\n",
    "\n",
    "                dataset = engel_1_2.sample(frac = 1)\n",
    "                y = dataset['resected'].to_numpy()\n",
    "                dataset = dataset.drop(['resected'], axis=1)\n",
    "                x = scaler.fit_transform(dataset)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "                clf.fit(x_train, y_train)\n",
    "                pred = clf.predict(x_test)\n",
    "\n",
    "                acc.append(accuracy_score(y_test, pred))\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0,1]).ravel()\n",
    "                if not any([tn + fp == 0, tp + fn == 0, tn + fn == 0, tp + fp == 0]):\n",
    "                    spec.append(tn / (tn + fp))\n",
    "                    sens.append(tp / (tp + fn))\n",
    "                    negpred.append(tn/(tn + fn))\n",
    "                    pospred.append(tp/(tp + fp))\n",
    "\n",
    "            curr_series.append(np.array(acc).mean())\n",
    "            print(f'{features[1]},\\n{features[0]}')\n",
    "            print('acc: ', np.array(acc).mean())\n",
    "            print('spec: ', np.array(spec).mean())\n",
    "            print('sens: ', np.array(sens).mean())\n",
    "        series.append(pd.Series(curr_series))\n",
    "\n",
    "    df = pd.DataFrame(series, index=index).rename(columns={i: col for i, col in enumerate(index)})\n",
    "    df.to_csv(f'/home/user/Documents/hemispheres_metrics_accuracy_{feat}.csv')\n",
    "    print(f'{features[1]}|{features[0]}')\n",
    "    print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wpli + transitivity & envelope + global efficiency\n",
    "\n",
    "subjects = [subject for subject in SUBJECTS if subject.name not in REJECTED]\n",
    "GRAPHS = [\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.cluster.transitivity),\n",
    "    # metric_for_hemispheres(subjects, nx.algorithms.smetric.s_metric, normalized=False),\n",
    "    metric_for_hemispheres(subjects, nx.algorithms.global_efficiency),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wpli = GRAPHS[0]['transitivity_for_wpli_4-8Hz']\n",
    "envelope = GRAPHS[1]['global_efficiency_for_envelope_4-8Hz']\n",
    "assert all(GRAPHS[0]['resected'] == GRAPHS[1]['resected'])\n",
    "labels = GRAPHS[0]['resected']\n",
    "dataset = pd.concat([wpli, envelope, labels], axis=1)\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "for _ in range(1000):\n",
    "    # Engel 1&2\n",
    "    engel_1_2 = dataset.copy().loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in dataset.index)]\n",
    "    engel_1_2 = engel_1_2.sample(frac=1)\n",
    "    y = engel_1_2['resected'].to_numpy()\n",
    "    x = engel_1_2.drop('resected', axis=1).to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.5)\n",
    "    # Engel 3&4\n",
    "    engel_3_4 = dataset.copy().loc[(elem[:4] in [*ENGEL34] and not elem[:4] in REJECTED for elem in dataset.index)].sample(frac=1)\n",
    "    y_engel_3_4 = engel_3_4['resected'].to_numpy()\n",
    "    x_engel_3_4 = engel_3_4.drop('resected', axis=1).to_numpy()\n",
    "    x_test = np.append(x_test, x_engel_3_4, axis=0)\n",
    "    y_test = np.append(y_test, y_engel_3_4, axis=0)\n",
    "\n",
    "\n",
    "    clf = svm.SVC(kernel='sigmoid')\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    # print(pred)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0,1]).ravel()\n",
    "    if not any([tn + fp == 0, tp + fn == 0, tn + fn == 0, tp + fp == 0]):\n",
    "        spec.append(tn / (tn + fp))\n",
    "        sens.append(tp / (tp + fn))\n",
    "        negpred.append(tn/(tn + fn))\n",
    "        pospred.append(tp/(tp + fp))\n",
    "spec = [s for s in spec if not np.isnan(s)]\n",
    "sens = [s for s in sens if not np.isnan(s)]\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "# regions wpli+eigen & envelope+percolation\n",
    "\n",
    "path = f'/home/user//Documents/initial_stats.pkl'\n",
    "stats = pickle.load(open(path, 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.5545774647887324\n",
      "spec:  0.5759884235299025\n",
      "sens:  0.5501672975761028\n"
     ]
    }
   ],
   "source": [
    "stat1 = stats[0]\n",
    "stat2 = stats[-1]\n",
    "\n",
    "full_true_data_1 = stat1.datasets['true']['4-8Hz_wpli']\n",
    "full_true_data_2 = stat2.datasets['true']['4-8Hz_envelope']\n",
    "full_false_data_1 = stat1.datasets['false_mirror']['4-8Hz_wpli']\n",
    "full_false_data_2 = stat2.datasets['false_mirror']['4-8Hz_envelope']\n",
    "\n",
    "full_true_data = pd.concat([full_true_data_1, full_true_data_2], axis=1)\n",
    "\n",
    "full_false_data = pd.concat([full_false_data_1, full_false_data_2], axis=1)\n",
    "\n",
    "\n",
    "features = ['4-8Hz_envelope', '4-8Hz_wpli']\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    clf = svm.SVC(kernel='sigmoid')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    ## All data\n",
    "    # true_data = full_true_data.assign(resected=True)\n",
    "    # false_data = full_false_data.assign(resected=False)\n",
    "\n",
    "    ## Engel1 only\n",
    "    # engel_1_true = full_true_data.loc[(elem[:4] in ENGEL1 for elem in full_true_data.index)]\n",
    "    # engel_1_false = full_false_data.loc[(elem[:4] in ENGEL1 for elem in full_false_data.index)]\n",
    "    # true_data = engel_1_true.assign(resected=True)\n",
    "    # false_data = engel_1_false.assign(resected=False)\n",
    "\n",
    "    ## Engel1&2 data\n",
    "    # engel_1_2_true = full_true_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_true_data.index)]\n",
    "    # engel_1_2_false = full_false_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_false_data.index)]\n",
    "    # true_data = engel_1_2_true.assign(resected=True)\n",
    "    # false_data = engel_1_2_false.assign(resected=False)\n",
    "    #\n",
    "    # dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    # dataset = dataset.sample(frac = 1)\n",
    "    #\n",
    "    # y = dataset['resected'].to_numpy()\n",
    "    # dataset = dataset.drop(['resected'], axis=1)\n",
    "    # x = scaler.fit_transform(dataset)\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    # Engel1 - train, the rest - test\n",
    "    engel_1_true = full_true_data.loc[(elem[:4] in ENGEL1 and not elem[:4] in REJECTED for elem in full_true_data.index)].assign(resected=True)\n",
    "    engel_1_false = full_false_data.loc[(elem[:4] in ENGEL1 and not elem[:4] in REJECTED for elem in full_false_data.index)].assign(resected=False)\n",
    "    engel_2_3_4_true = full_true_data\\\n",
    "        .loc[(elem[:4] not in ENGEL1 and not elem[:4] in REJECTED for elem in full_true_data.index)].assign(resected=True)\n",
    "    engel_2_3_4_false = full_false_data\\\n",
    "        .loc[(elem[:4] not in ENGEL1 and not elem[:4] in REJECTED for elem in full_false_data.index)].assign(resected=False)\n",
    "    engel_1 = pd.concat([engel_1_true, engel_1_false], axis=0)\n",
    "    engel_1_train, engel_1_test = np.array_split(engel_1.sample(frac=1), 2)\n",
    "    engel_2_3_4 = pd.concat([engel_2_3_4_true, engel_2_3_4_false], axis=0)\n",
    "\n",
    "    y_train = engel_1_train['resected'].to_numpy()\n",
    "    x_train = scaler.fit_transform(engel_1_train.drop(['resected'], axis=1))\n",
    "\n",
    "    engel_test = pd.concat([engel_2_3_4, engel_1_test], axis=0).sample(frac=1)\n",
    "\n",
    "    y_test = engel_test['resected'].to_numpy()\n",
    "    x_test = scaler.fit_transform(engel_test.drop(['resected'], axis=1))\n",
    "\n",
    "\n",
    "    # # Engel1&2 - train, the rest - test\n",
    "    # engel_1_2_true = full_true_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_true_data.index)].assign(resected=True)\n",
    "    # engel_1_2_false = full_false_data.loc[(elem[:4] in [*ENGEL1, *ENGEL2] and not elem[:4] in REJECTED for elem in full_false_data.index)].assign(resected=False)\n",
    "    # engel_3_4_true = full_true_data\\\n",
    "    #     .loc[(elem[:4] not in [*ENGEL1, *ENGEL2, *REJECTED] for elem in full_true_data.index)].assign(resected=True)\n",
    "    # engel_3_4_false = full_false_data\\\n",
    "    #     .loc[(elem[:4] not in [*ENGEL1, *ENGEL2, *REJECTED] for elem in full_false_data.index)].assign(resected=False)\n",
    "    # engel_1_2 = pd.concat([engel_1_2_true, engel_1_2_false], axis=0)\n",
    "    # engel_1_2_train, engel_1_2_test = np.array_split(engel_1_2.sample(frac=1), 2)\n",
    "    # engel_3_4 = pd.concat([engel_3_4_true, engel_3_4_false], axis=0)\n",
    "    #\n",
    "    # y_train = engel_1_2_train['resected'].to_numpy()\n",
    "    # x_train = scaler.fit_transform(engel_1_2_train.drop(['resected'], axis=1))\n",
    "    #\n",
    "    # engel_test = pd.concat([engel_3_4, engel_1_2_test], axis=0).sample(frac=1)\n",
    "    #\n",
    "    # y_test = engel_test['resected'].to_numpy()\n",
    "    # x_test = scaler.fit_transform(engel_test.drop(['resected'], axis=1))\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0,1]).ravel()\n",
    "    if not any([tn + fp == 0, tp + fn == 0, tn + fn == 0, tp + fp == 0]):\n",
    "        spec.append(tn / (tn + fp))\n",
    "        sens.append(tp / (tp + fn))\n",
    "        negpred.append(tn/(tn + fn))\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "rois = [50, 77, 74, 72, 50, 57, 57, 62, 64, 54, 56, 17, 50, 88, 55, 57, 40,]# 39, 63, 41]\n",
    "hemis = [59, 95, 98, 50, 48, 48, 97, 73, 93, 95, 96, 36, 72, 45, 64, 68, 60,]# 3, 71, 41]\n",
    "\n",
    "# plt.plot(rois, hemis, 'x')\n",
    "# plt.show()\n",
    "# plt.plot(hemis, rois, 'x')\n",
    "# plt.xlabel('Hemispheres accuracy, %')\n",
    "# plt.ylabel('ROIs accuracy, %')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.29839908228635526, pvalue=0.2446825909683375)\n",
      "(0.31864598766929964, 0.21256350182108505)\n"
     ]
    }
   ],
   "source": [
    "print(sp.stats.spearmanr(rois, hemis))\n",
    "print(sp.stats.pearsonr(rois, hemis))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}