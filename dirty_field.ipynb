{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from abc import *\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import mne\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors._dist_metrics import DistanceMetric\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nodestimation.learning.estimation import collect_statistic, \\\n",
    "    compute_importance, collect_cross_statistic, make_selection_map, \\\n",
    "    select, separate_datasets, selected_statistic, choose_best, selected_data, make_feature_selection\n",
    "from nodestimation.learning.informativeness import CrossInformativeness, Informativeness, SubjectsInformativeness, \\\n",
    "    NodesInformativeness\n",
    "from nodestimation.learning.networking import sparse_graph, graph_to_hemispheres, hemispheres_division_modularity, \\\n",
    "    metric_for_hemispheres\n",
    "from nodestimation.processing.features import prepare_features\n",
    "from nodestimation.project import find_subject_dir, conditions_unique_code\n",
    "from nodestimation.pipeline import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nibabel\n",
    "import nilearn.plotting as nplt\n",
    "from nodestimation.project.actions import read\n",
    "import nodestimation as nd\n",
    "from nodestimation.learning.modification import append_series, promote\n",
    "import nodestimation.learning.modification as lmd\n",
    "from nodestimation.project.subject import Subject\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from nodestimation.learning.selection import SubjectsStatistic, Wilcoxon, Mannwhitneyu, Test\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.fftpack import fftfreq, irfft, rfft\n",
    "from scipy.fftpack import fftfreq, irfft, rfft\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# subjects = pipeline(\n",
    "#     methods=['wpli', 'envelope'],\n",
    "#     freq_bands=(7.5, 12),\n",
    "#     centrality_metrics=['eigen', 'close', 'between', 'degree', ],# 'katz', 'info', 'harmonic']\n",
    "#     subjects_specificity={\n",
    "#         'M2S2': {\n",
    "#             'freq_bands': (7.5, 12.5)\n",
    "#         },\n",
    "#         'R1D2': {\n",
    "#             'freq_bands': (7.5, 11)\n",
    "#         },\n",
    "#         'S1A2': {\n",
    "#             'freq_bands': (5, 10)\n",
    "#         },\n",
    "#         'S1H1': {\n",
    "#             'freq_bands': (8, 13)\n",
    "#         },\n",
    "#         'K1V1': {\n",
    "#             'freq_bands': (7.5, 11)\n",
    "#         },\n",
    "#         'L1P1': {\n",
    "#             'freq_bands': (5, 10)\n",
    "#         },\n",
    "#         'M1G2': {\n",
    "#             'freq_bands': (7, 11)\n",
    "#         },\n",
    "#         'G1V2': {\n",
    "#             'freq_bands': (7, 11)\n",
    "#         },\n",
    "#         'G1R1': {\n",
    "#             'freq_bands': (12.5, 16.5)\n",
    "#         },\n",
    "#         'M1N2': {\n",
    "#             'freq_bands': (10, 15)\n",
    "#         },\n",
    "#         'B1R1': {\n",
    "#             'freq_bands': (6, 11)\n",
    "#         },\n",
    "#         'B1C2': {\n",
    "#             'freq_bands': (7.5, 12.5)\n",
    "#         },\n",
    "#         'J1T2': {\n",
    "#             'freq_bands': (11, 15)\n",
    "#         },\n",
    "#         'O1O2': {\n",
    "#             'freq_bands': (5.5, 9.5)\n",
    "#         },\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# subjects = pipeline(\n",
    "#     methods=['wpli', 'envelope'],\n",
    "#     freq_bands=(4, 8),\n",
    "#     centrality_metrics=['eigen', 'close', 'between', 'degree']\n",
    "#     )\n",
    "\n",
    "subjects = pipeline(\n",
    "    methods=['wpli', 'envelope', 'coh', 'imcoh', 'plv', 'ciplv', 'ppc', 'pli', 'pli2_unbiased', 'wpli2_debiased'],\n",
    "    freq_bands=(4, 8),\n",
    "    centrality_metrics=['eigen', 'between', 'degree', 'info']\n",
    "    )\n",
    "\n",
    "# for subject in subjects:\n",
    "#     for dataset in subject.datasets:\n",
    "#         print(subject.datasets[dataset].head())\n",
    "        # print(dataset)\n",
    "        # print(subject.dataset[dataset])\n",
    "        # columns = subject.dataset[dataset].columns.to_list()\n",
    "        # rule = dict()\n",
    "        # for column in columns:\n",
    "        #     if 'wpli' in column:\n",
    "        #         rule.update({column: 'wpli'})\n",
    "        #     if 'envelope' in column:\n",
    "        #         rule.update({column: 'envelope'})\n",
    "        # subject.dataset[dataset] = subject.dataset[dataset].rename(columns=rule, copy=True)\n",
    "\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    subject: {\n",
    "        freq: {\n",
    "            method: lmd.suppress(\n",
    "                    subject.connectomes[freq][method],\n",
    "                    trigger=subject.connectomes[freq][method].mean().mean(),\n",
    "                    optimal=0\n",
    "                )\n",
    "            # method: subject.connectomes[freq][method]\n",
    "           for method in subject.connectomes[freq]\n",
    "        }\n",
    "        for freq in subject.connectomes\n",
    "    }\n",
    "    for subject in subjects\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(subjects))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N2K2 4-8Hz ciplv\n",
      "N2K2 4-8Hz ppc\n",
      "N2K2 4-8Hz pli\n",
      "N2K2 4-8Hz pli2_unbiased\n",
      "N2K2 4-8Hz wpli2_debiased\n",
      "K4L2 4-8Hz wpli\n",
      "K4L2 4-8Hz envelope\n",
      "K4L2 4-8Hz coh\n",
      "K4L2 4-8Hz imcoh\n",
      "K4L2 4-8Hz plv\n",
      "K4L2 4-8Hz ciplv\n",
      "K4L2 4-8Hz ppc\n",
      "K4L2 4-8Hz pli\n",
      "K4L2 4-8Hz pli2_unbiased\n",
      "K4L2 4-8Hz wpli2_debiased\n",
      "B1C2 4-8Hz wpli\n",
      "B1C2 4-8Hz envelope\n",
      "B1C2 4-8Hz coh\n",
      "B1C2 4-8Hz imcoh\n",
      "B1C2 4-8Hz plv\n",
      "B1C2 4-8Hz ciplv\n",
      "B1C2 4-8Hz ppc\n",
      "B1C2 4-8Hz pli\n",
      "B1C2 4-8Hz pli2_unbiased\n",
      "B1C2 4-8Hz wpli2_debiased\n",
      "J1T2 4-8Hz wpli\n",
      "J1T2 4-8Hz envelope\n",
      "J1T2 4-8Hz coh\n",
      "J1T2 4-8Hz imcoh\n",
      "J1T2 4-8Hz plv\n",
      "J1T2 4-8Hz ciplv\n",
      "J1T2 4-8Hz ppc\n",
      "J1T2 4-8Hz pli\n",
      "J1T2 4-8Hz pli2_unbiased\n",
      "J1T2 4-8Hz wpli2_debiased\n",
      "O1O2 4-8Hz wpli\n",
      "O1O2 4-8Hz envelope\n",
      "O1O2 4-8Hz coh\n",
      "O1O2 4-8Hz imcoh\n",
      "O1O2 4-8Hz plv\n",
      "O1O2 4-8Hz ciplv\n",
      "O1O2 4-8Hz ppc\n",
      "O1O2 4-8Hz pli\n",
      "O1O2 4-8Hz pli2_unbiased\n",
      "O1O2 4-8Hz wpli2_debiased\n",
      "L2M1 4-8Hz wpli\n",
      "L2M1 4-8Hz envelope\n",
      "L2M1 4-8Hz coh\n",
      "L2M1 4-8Hz imcoh\n",
      "L2M1 4-8Hz plv\n",
      "L2M1 4-8Hz ciplv\n",
      "L2M1 4-8Hz ppc\n",
      "L2M1 4-8Hz pli\n",
      "L2M1 4-8Hz pli2_unbiased\n",
      "L2M1 4-8Hz wpli2_debiased\n",
      "All is done\n"
     ]
    }
   ],
   "source": [
    "close, between, eigen, degree, info, harmony = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "for subject in datasets:\n",
    "    close.update({subject.name: dict()})\n",
    "    between.update({subject.name: dict()})\n",
    "    eigen.update({subject.name: dict()})\n",
    "    degree.update({subject.name: dict()})\n",
    "    info.update({subject.name: dict()})\n",
    "    harmony.update({subject.name: dict()})\n",
    "    # katz.update({subject.name: dict()})\n",
    "    for freq in datasets[subject]:\n",
    "        for method in datasets[subject][freq]:\n",
    "            print(subject.name, freq, method)\n",
    "            label_names = datasets[subject][freq][method].columns\n",
    "            if nx.is_connected(\n",
    "                    nx.convert_matrix.from_numpy_array(\n",
    "                        datasets[subject][freq][method].to_numpy()\n",
    "                    )\n",
    "            ):\n",
    "                arr  = datasets[subject][freq][method].to_numpy()\n",
    "            elif nx.is_connected(\n",
    "                    nx.convert_matrix.from_numpy_array(\n",
    "                        subject.connectomes[freq][method].to_numpy()\n",
    "                    )\n",
    "            ):\n",
    "                arr = subject.connectomes[freq][method].to_numpy()\n",
    "            else:\n",
    "                raise ValueError(f'Graph not connected: {subject.name}, {freq}, {method}')\n",
    "            if arr.min().min() < 0:\n",
    "                for i in range(arr.shape[0]):\n",
    "                    for j in range(arr.shape[1]):\n",
    "                        if arr[i, j] != 0: arr[i, j] -= arr.min().min()\n",
    "            G = nx.convert_matrix.from_numpy_array(arr)\n",
    "            mapping = {node: label_name for node, label_name in zip(G, label_names)}\n",
    "            G = nx.relabel_nodes(G, mapping)\n",
    "            for place, data in zip(\n",
    "                [\n",
    "                    close[subject.name],\n",
    "                    between[subject.name],\n",
    "                    eigen[subject.name],\n",
    "                    info[subject.name],\n",
    "                    harmony[subject.name],\n",
    "                    degree[subject.name],\n",
    "                    # katz[subject.name]\n",
    "                ],[\n",
    "                        nx.closeness_centrality(G, distance='weight'),\n",
    "                        nx.betweenness_centrality(G, weight='weight'),\n",
    "                        nx.eigenvector_centrality_numpy(G, weight='weight'),\n",
    "                        nx.information_centrality(G, weight='weight'),\n",
    "                        nx.harmonic_centrality(G, distance='weight'),\n",
    "                        dict(G.degree(weight='weight')),\n",
    "                        # nx.katz_centrality(G, weight='weight', max_iter=100)\n",
    "                    ]\n",
    "            ):\n",
    "                place.update({\n",
    "                    freq + '_' + method: pd.Series(data)\n",
    "                })\n",
    "\n",
    "print('All is done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "datasets_centrality = dict()\n",
    "\n",
    "for subject in subjects:\n",
    "    datasets_centrality.update({subject.name: dict()})\n",
    "    datasets_centrality[subject.name].update({\n",
    "        'close': pd.DataFrame(close[subject.name]),\n",
    "        'between': pd.DataFrame(between[subject.name]),\n",
    "        'eigen': pd.DataFrame(eigen[subject.name]),\n",
    "        'degree': pd.DataFrame(degree[subject.name]),\n",
    "        'info': pd.DataFrame(info[subject.name]),\n",
    "        'harmony': pd.DataFrame(harmony[subject.name]),\n",
    "        # 'katz': pd.DataFrame(katz[subject.name])\n",
    "    })\n",
    "    true = subject.datasets['eigen']['resected']\n",
    "    datasets_centrality[subject.name]['close'] = datasets_centrality[subject.name]['close'].assign(resected=true)\n",
    "    datasets_centrality[subject.name]['between'] = datasets_centrality[subject.name]['between'].assign(resected=true)\n",
    "    datasets_centrality[subject.name]['eigen'] = datasets_centrality[subject.name]['eigen'].assign(resected=true)\n",
    "    datasets_centrality[subject.name]['degree'] = datasets_centrality[subject.name]['degree'].assign(resected=true)\n",
    "    datasets_centrality[subject.name]['info'] = datasets_centrality[subject.name]['info'].assign(resected=true)\n",
    "    datasets_centrality[subject.name]['harmony'] = datasets_centrality[subject.name]['harmony'].assign(resected=true)\n",
    "    # datasets_centrality[subject.name]['katz'] = datasets_centrality[subject.name]['katz'].assign(resected=true)\n",
    "\n",
    "for subject in subjects:\n",
    "    subject.datasets = datasets_centrality[subject.name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1\n",
      "done 2\n",
      "done 3\n",
      "done 4\n",
      "done 5\n",
      "done 6\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "stat1 = SubjectsStatistic(subjects, 'resected', centrality_metric='eigen')\n",
    "print('done 1')\n",
    "stat2 = SubjectsStatistic(subjects, 'resected', centrality_metric='between')\n",
    "print('done 2')\n",
    "stat3 = SubjectsStatistic(subjects, 'resected', centrality_metric='close')\n",
    "print('done 3')\n",
    "stat4 = SubjectsStatistic(subjects, 'resected', centrality_metric='degree')\n",
    "print('done 4')\n",
    "stat5 = SubjectsStatistic(subjects, 'resected', centrality_metric='info')\n",
    "print('done 5')\n",
    "stat6 = SubjectsStatistic(subjects, 'resected', centrality_metric='harmony')\n",
    "print('done 6')\n",
    "# stat7 = SubjectsStatistic(subjects, 'resected', centrality_metric='katz')\n",
    "print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1 = stat1.test(state='reflected')\n",
    "test2 = stat2.test(state='reflected')\n",
    "test3 = stat3.test(state='reflected')\n",
    "test4 = stat4.test(state='reflected')\n",
    "test5 = stat5.test(state='reflected')\n",
    "test6 = stat6.test(state='reflected')\n",
    "test1_samples, test2_samples, test3_samples, test4_samples, test5_samples, test6_samples = list(), list(), list(), list(), list(), list()\n",
    "for feature in test1.result:\n",
    "    test1_samples.append(test1.result[feature][1])\n",
    "    test2_samples.append(test2.result[feature][1])\n",
    "    test3_samples.append(test3.result[feature][1])\n",
    "    test4_samples.append(test4.result[feature][1])\n",
    "    test5_samples.append(test5.result[feature][1])\n",
    "    test6_samples.append(test6.result[feature][1])\n",
    "\n",
    "test_samples = np.array([\n",
    "    np.array(test1_samples),\n",
    "    np.array(test2_samples),\n",
    "    np.array(test3_samples),\n",
    "    np.array(test4_samples),\n",
    "    np.array(test5_samples),\n",
    "    np.array(test6_samples)\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(test_samples, columns=list(test1.result.keys()), index=[\n",
    "    'eigen', 'between', 'close', 'degree', 'info', 'harmony'\n",
    "]).T\n",
    "print(df)\n",
    "df.to_csv('/home/user/Documents/Wilcoxon.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1 = stat1.test(state='reflected', test='mannwhitneyu')\n",
    "test2 = stat2.test(state='reflected', test='mannwhitneyu')\n",
    "test3 = stat3.test(state='reflected', test='mannwhitneyu')\n",
    "test4 = stat4.test(state='reflected', test='mannwhitneyu')\n",
    "test5 = stat5.test(state='reflected', test='mannwhitneyu')\n",
    "test6 = stat6.test(state='reflected', test='mannwhitneyu')\n",
    "test1_samples, test2_samples, test3_samples, test4_samples, test5_samples, test6_samples = list(), list(), list(), list(), list(), list()\n",
    "for feature in test1.result:\n",
    "    test1_samples.append(test1.result[feature][1])\n",
    "    test2_samples.append(test2.result[feature][1])\n",
    "    test3_samples.append(test3.result[feature][1])\n",
    "    test4_samples.append(test4.result[feature][1])\n",
    "    test5_samples.append(test5.result[feature][1])\n",
    "    test6_samples.append(test6.result[feature][1])\n",
    "\n",
    "test_samples = np.array([\n",
    "    np.array(test1_samples),\n",
    "    np.array(test2_samples),\n",
    "    np.array(test3_samples),\n",
    "    np.array(test4_samples),\n",
    "    np.array(test5_samples),\n",
    "    np.array(test6_samples)\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(test_samples, columns=list(test1.result.keys()), index=[\n",
    "    'eigen', 'between', 'close', 'degree', 'info', 'harmony'\n",
    "]).T\n",
    "print(df)\n",
    "df.to_csv('/home/user/Documents/MannWhitneyu.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.imshow(\n",
    "#     pd.DataFrame(\n",
    "#         nx.to_numpy_matrix(\n",
    "#             subjects[0].graph['7.5-12.5Hz']['wpli']\n",
    "#         )\n",
    "#     ).to_numpy())\n",
    "# plt.show()\n",
    "# plt.imshow(\n",
    "#     lmd.suppress(\n",
    "#         pd.DataFrame(\n",
    "#             nx.to_numpy_matrix(\n",
    "#                 subjects[0].graph['7.5-12.5Hz']['wpli']\n",
    "#             )\n",
    "#         ),\n",
    "#         optimal=0\n",
    "#     ).to_numpy())\n",
    "# plt.show()\n",
    "#\n",
    "G = sparse_graph(subjects[0].graph['7.5-12.5Hz']['wpli'])\n",
    "lh, rh = graph_to_hemispheres(G)\n",
    "print(lh.number_of_edges(), lh.number_of_nodes())\n",
    "# nx.draw(subjects[0].graph['7.5-12.5Hz']['wpli'])\n",
    "# plt.show()\n",
    "# nx.draw(G)\n",
    "# plt.show()\n",
    "# nx.draw(lh)\n",
    "# plt.show()\n",
    "# nx.draw(rh)\n",
    "# plt.show()\n",
    "# print('all: ', smallworldness(G))\n",
    "import time\n",
    "start = time.time()\n",
    "# print(smallworldness(nx.complete_graph(5)))\n",
    "# print(time.time() - start)\n",
    "# print(smallworldness(nx.complete_graph(10)))\n",
    "# print(time.time() - start)\n",
    "# print(smallworldness(nx.complete_graph(75)))\n",
    "# print(time.time() - start)\n",
    "# print('lh: ', smallworldness(lh))\n",
    "# print('rh: ', smallworldness(rh))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    print(subject.name)\n",
    "    print('\\n\\tresected nodes:')\n",
    "    for node in subject.nodes:\n",
    "        if node.type == 'resected':\n",
    "            print(f'\\t{node.label.name}')\n",
    "    for freq in subject.connectomes:\n",
    "        for method in subject.connectomes[freq]:\n",
    "            print(f'\\n\\t{freq}: {method}')\n",
    "            label_names = list(subject.connectomes[freq][method].index)\n",
    "            mapping = {\n",
    "                i: label_name\n",
    "                for i, label_name in zip(\n",
    "                    range(len(label_names)),\n",
    "                    label_names\n",
    "                )\n",
    "            }\n",
    "            G = sparse_graph(\n",
    "                nx.convert_matrix.from_numpy_matrix(\n",
    "                    subject.connectomes[freq][method].to_numpy()\n",
    "                )\n",
    "            )\n",
    "            G = nx.relabel_nodes(G, mapping)\n",
    "            lh, rh = graph_to_hemispheres(G)\n",
    "            print(f'\\n\\themispheres division modularity: {hemispheres_division_modularity(G)}\\n')\n",
    "            print('\\n\\t s for lh', nx.algorithms.smetric.s_metric(lh, normalized=False)/100000)\n",
    "            print('\\n\\t s for rh', nx.algorithms.smetric.s_metric(rh, normalized=False)/100000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1H2: DONE, RUNTIME: 3.446002721786499\n",
      "M2S2: DONE, RUNTIME: 3.3521647453308105\n",
      "R1D2: DONE, RUNTIME: 3.8036186695098877\n",
      "N3S2: DONE, RUNTIME: 3.0101189613342285\n",
      "S1A2: DONE, RUNTIME: 2.8685755729675293\n",
      "S1H1: DONE, RUNTIME: 4.034933805465698\n",
      "K1V1: DONE, RUNTIME: 3.8956894874572754\n",
      "L1P1: DONE, RUNTIME: 3.00840425491333\n",
      "M1G2: DONE, RUNTIME: 3.700443744659424\n",
      "G1V2: DONE, RUNTIME: 2.778379201889038\n",
      "G1R1: DONE, RUNTIME: 2.794992685317993\n",
      "M1N2: DONE, RUNTIME: 3.0254180431365967\n",
      "S1B1: DONE, RUNTIME: 3.462425708770752\n",
      "S1U3: DONE, RUNTIME: 2.9969425201416016\n",
      "B1R1: DONE, RUNTIME: 2.899250030517578\n",
      "S3R1: DONE, RUNTIME: 3.356846809387207\n",
      "N2K2: DONE, RUNTIME: 5.198939800262451\n",
      "K4L2: DONE, RUNTIME: 3.3211276531219482\n",
      "B1C2: DONE, RUNTIME: 3.375744342803955\n",
      "J1T2: DONE, RUNTIME: 3.3323800563812256\n",
      "O1O2: DONE, RUNTIME: 3.3152332305908203\n",
      "L2M1: DONE, RUNTIME: 3.8451991081237793\n",
      "P1H2: DONE, RUNTIME: 2.136042833328247\n",
      "M2S2: DONE, RUNTIME: 2.086578130722046\n",
      "R1D2: DONE, RUNTIME: 2.2747559547424316\n",
      "N3S2: DONE, RUNTIME: 1.9102375507354736\n",
      "S1A2: DONE, RUNTIME: 1.7360711097717285\n",
      "S1H1: DONE, RUNTIME: 2.368006706237793\n",
      "K1V1: DONE, RUNTIME: 2.3574819564819336\n",
      "L1P1: DONE, RUNTIME: 1.8679516315460205\n",
      "M1G2: DONE, RUNTIME: 2.2915380001068115\n",
      "G1V2: DONE, RUNTIME: 1.7500195503234863\n",
      "G1R1: DONE, RUNTIME: 1.7607910633087158\n",
      "M1N2: DONE, RUNTIME: 1.9344627857208252\n",
      "S1B1: DONE, RUNTIME: 2.063095808029175\n",
      "S1U3: DONE, RUNTIME: 1.8045344352722168\n",
      "B1R1: DONE, RUNTIME: 1.7584407329559326\n",
      "S3R1: DONE, RUNTIME: 2.122227907180786\n",
      "N2K2: DONE, RUNTIME: 3.0724589824676514\n",
      "K4L2: DONE, RUNTIME: 2.052717447280884\n",
      "B1C2: DONE, RUNTIME: 2.106555461883545\n",
      "J1T2: DONE, RUNTIME: 2.01438570022583\n",
      "O1O2: DONE, RUNTIME: 2.0458569526672363\n",
      "L2M1: DONE, RUNTIME: 2.3689050674438477\n",
      "P1H2: DONE, RUNTIME: 2.344226837158203\n",
      "M2S2: DONE, RUNTIME: 2.246518850326538\n",
      "R1D2: DONE, RUNTIME: 2.4860007762908936\n",
      "N3S2: DONE, RUNTIME: 1.9057424068450928\n",
      "S1A2: DONE, RUNTIME: 1.8468899726867676\n",
      "S1H1: DONE, RUNTIME: 2.5233588218688965\n",
      "K1V1: DONE, RUNTIME: 2.5011889934539795\n",
      "L1P1: DONE, RUNTIME: 1.9890334606170654\n",
      "M1G2: DONE, RUNTIME: 2.4134132862091064\n",
      "G1V2: DONE, RUNTIME: 1.8330872058868408\n",
      "G1R1: DONE, RUNTIME: 1.9589312076568604\n",
      "M1N2: DONE, RUNTIME: 2.136899471282959\n",
      "S1B1: DONE, RUNTIME: 2.343425989151001\n",
      "S1U3: DONE, RUNTIME: 1.915477991104126\n",
      "B1R1: DONE, RUNTIME: 1.8502538204193115\n",
      "S3R1: DONE, RUNTIME: 2.2278828620910645\n",
      "N2K2: DONE, RUNTIME: 3.1519253253936768\n",
      "K4L2: DONE, RUNTIME: 2.277439594268799\n",
      "B1C2: DONE, RUNTIME: 2.2829172611236572\n",
      "J1T2: DONE, RUNTIME: 2.3673994541168213\n",
      "O1O2: DONE, RUNTIME: 2.26993465423584\n",
      "L2M1: DONE, RUNTIME: 2.4328744411468506\n"
     ]
    }
   ],
   "source": [
    "dataset1 = metric_for_hemispheres(subjects, nx.algorithms.cluster.transitivity)\n",
    "dataset2 = metric_for_hemispheres(subjects, nx.algorithms.smetric.s_metric, normalized=False)\n",
    "dataset3 = metric_for_hemispheres(subjects, nx.algorithms.global_efficiency)\n",
    "# dataset = metric_for_hemispheres(subjects, nx.algorithms.smetric.s_metric, normalized=False)\n",
    "# dataset = metric_for_hemispheres(subjects, nx.algorithms.global_efficiency)\n",
    "# dataset = s_for_hemispheres(subjects)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(dataset[['transitivity_for_wpli_4-8Hz', 'transitivity_for_envelope_4-8Hz', 'resected']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets = [dataset1, dataset2, dataset3]\n",
    "names = ['transitivity', 's_metric', 'global_efficiency']\n",
    "cross_hemispheres_informativeness_arr = list()\n",
    "cross_subjects_informativeness_arr = list()\n",
    "\n",
    "for dataset, name in zip(datasets, names):\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "    for i in range(1000):\n",
    "\n",
    "        y = dataset['resected'].to_numpy()\n",
    "        # x = dataset.drop(['resected'], axis=1).to_numpy()\n",
    "        # x = dataset[['s_metric_for_wpli_4-8Hz', 's_metric_for_envelope_4-8Hz']].to_numpy()\n",
    "        x = dataset[[f'{name}_for_wpli_4-8Hz', f'{name}_for_envelope_4-8Hz']].to_numpy()\n",
    "        # x = dataset[['global_efficiency_for_wpli_4-8Hz', 'global_efficiency_for_envelope_4-8Hz']].to_numpy()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        # scaler = MinMaxScaler()\n",
    "        # scaler = MaxAbsScaler()\n",
    "        # scaler = RobustScaler()\n",
    "        x = scaler.fit_transform(x)\n",
    "\n",
    "        samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "        x = np.append(x, samples, axis=1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "        train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "        x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "        # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "        # clf = AdaBoostClassifier(n_estimators=10)\n",
    "        # clf = svm.SVC(kernel='sigmoid')\n",
    "        clf = svm.SVC()\n",
    "        # clf = svm.SVC(kernel='linear', class_weight={True: .8, False: 1})\n",
    "        # clf = svm.SVC(kernel='sigmoid', class_weight={True: .8, False: 1})\n",
    "        # clf = svm.SVC(class_weight={True: .8, False: 1})\n",
    "        # clf = SGDClassifier()\n",
    "        # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "        # clf = LogisticRegression(class_weight={True: .7, False: 1})\n",
    "        # clf = RandomForestClassifier(max_depth=20)\n",
    "        # clf = GaussianNB()\n",
    "        # clf = LinearDiscriminantAnalysis()\n",
    "        # clf = QuadraticDiscriminantAnalysis()\n",
    "        # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "        # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(10, 10), max_iter=1450)\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        pred = clf.predict(x_test)\n",
    "\n",
    "        acc.append(accuracy_score(y_test, pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "        spec.append(tn / (tn + fp))\n",
    "        sens.append(tp / (tp + fn))\n",
    "\n",
    "        if np.isnan(tp/(tp + fp)):\n",
    "            pospred.append(0)\n",
    "        else:\n",
    "            pospred.append(tp/(tp + fp))\n",
    "\n",
    "        if np.isnan((tn/(tn + fn))):\n",
    "            negpred.append(0)\n",
    "        else:\n",
    "            negpred.append(tn/(tn + fn))\n",
    "\n",
    "    print(f'acc: {np.array(acc).mean()}')\n",
    "    print(f'sens: {np.array(pospred).mean()}')\n",
    "    print(f'spec: {np.array(negpred).mean()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "datasets = [dataset1, dataset2, dataset3]\n",
    "names = ['transitivity', 's_metric', 'global_efficiency']\n",
    "cross_hemispheres_informativeness_arr = list()\n",
    "cross_subjects_informativeness_arr = list()\n",
    "\n",
    "for dataset, name in zip(datasets, names):\n",
    "    cross_hemispheres_informativeness = CrossInformativeness()\n",
    "    cross_subjects_informativeness = CrossInformativeness()\n",
    "\n",
    "    for _ in range(100):\n",
    "        hemispheres_informatoveness = Informativeness()\n",
    "        subjects_informativeness = SubjectsInformativeness()\n",
    "        acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "        for i in range(100):\n",
    "\n",
    "            y = dataset['resected'].to_numpy()\n",
    "            # x = dataset.drop(['resected'], axis=1).to_numpy()\n",
    "            # x = dataset[['s_metric_for_wpli_4-8Hz', 's_metric_for_envelope_4-8Hz']].to_numpy()\n",
    "            x = dataset[[f'{name}_for_wpli_4-8Hz', f'{name}_for_envelope_4-8Hz']].to_numpy()\n",
    "            # x = dataset[['global_efficiency_for_wpli_4-8Hz', 'global_efficiency_for_envelope_4-8Hz']].to_numpy()\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            # scaler = MinMaxScaler()\n",
    "            # scaler = MaxAbsScaler()\n",
    "            # scaler = RobustScaler()\n",
    "            x = scaler.fit_transform(x)\n",
    "\n",
    "            samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "            x = np.append(x, samples, axis=1)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "            train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "            x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "            # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "            # clf = AdaBoostClassifier(n_estimators=10)\n",
    "            clf = svm.SVC()\n",
    "            # clf = svm.SVC(kernel='sigmoid')\n",
    "            # clf = svm.SVC(kernel='linear', class_weight={True: .8, False: 1})\n",
    "            # clf = svm.SVC(kernel='sigmoid', class_weight={True: .8, False: 1})\n",
    "            # clf = svm.SVC(class_weight={True: .8, False: 1})\n",
    "            # clf = SGDClassifier()\n",
    "            # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "            # clf = LogisticRegression(class_weight={True: .8, False: 1})\n",
    "            # clf = RandomForestClassifier(max_depth=20)\n",
    "            # clf = GaussianNB()\n",
    "            # clf = LinearDiscriminantAnalysis()\n",
    "            # clf = QuadraticDiscriminantAnalysis()\n",
    "            # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "            # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(10, 10), max_iter=1450)\n",
    "\n",
    "            clf.fit(x_train, y_train)\n",
    "            pred = clf.predict(x_test)\n",
    "\n",
    "            for predicted, actual, sample, value in zip(pred, y_test, test_samples, x_test):\n",
    "                hemispheres_informatoveness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "                subjects_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "\n",
    "        cross_subjects_informativeness.informativeness = subjects_informativeness\n",
    "        cross_hemispheres_informativeness.informativeness = hemispheres_informatoveness\n",
    "\n",
    "    cross_hemispheres_informativeness_arr.append(cross_hemispheres_informativeness)\n",
    "    cross_subjects_informativeness_arr.append(cross_subjects_informativeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    sorted(\n",
    "        cross_subjects_informativeness_arr[0].ppv().items(),\n",
    "        key=lambda item: item[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.bar(data.keys(), data.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Subjects-level positive prediction value (for hemispheres)')\n",
    "plt.show()\n",
    "\n",
    "data = dict(\n",
    "    sorted(\n",
    "        cross_hemispheres_informativeness_arr[0].ppv().items(),\n",
    "        key=lambda item: item[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.bar(data.keys(), data.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Hemispheres-level positive prediction value, relatively to subject')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cross_nodes_informativeness = CrossInformativeness()\n",
    "cross_subjects_informativeness = CrossInformativeness()\n",
    "cross_samples_informativeness = CrossInformativeness()\n",
    "\n",
    "for _ in range(100):\n",
    "    # features = ['0.5-4Hz_wpli', '4-7Hz_wpli', '7-14Hz_wpli', '14-30Hz_wpli', '30-70Hz_wpli']\n",
    "    # features = ['14-30Hz_wpli', '4-7Hz_wpli']\n",
    "    # features = ['4-7Hz_wpli', '0.5-4Hz_envelope', '4-7Hz_envelope', '7-14Hz_envelope', '14-30Hz_envelope', '30-70Hz_envelope']\n",
    "    # features = ['4-7Hz_wpli', '4-7Hz_envelope']\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    # features = ['4-8Hz_wpli2_debiased', '4-8Hz_envelope']\n",
    "    # features = ['4-8Hz_wpli']\n",
    "    # features = ['4-7Hz_envelope']\n",
    "    # features = ['4-7Hz_wpli', '4-7Hz_psd']\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "    stat = stat1\n",
    "\n",
    "    stat.datasets['true']['4-8Hz_envelope'] = stat2.datasets['true']['4-8Hz_envelope']\n",
    "    stat.datasets['false_mirror']['4-8Hz_envelope'] = stat2.datasets['false_mirror']['4-8Hz_envelope']\n",
    "\n",
    "    samples_informativeness = Informativeness()\n",
    "    nodes_informativeness = NodesInformativeness()\n",
    "    subject_informativeness = SubjectsInformativeness()\n",
    "\n",
    "    for i in range(100):\n",
    "        # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "        # clf = AdaBoostClassifier(n_estimators=10)\n",
    "        # clf = svm.SVC(class_weight={True: .9, False: 1})\n",
    "        clf = svm.SVC(kernel='sigmoid')\n",
    "        # clf = svm.SVC(class_weight={True: 1, False: .8}, probability=True)\n",
    "        # clf = SGDClassifier()\n",
    "        # clf = KNeighborsClassifier(n_neighbors=7, metric='chebyshev')\n",
    "        # clf = LogisticRegression(class_weight={True: 1, False: .8})\n",
    "        # clf = LogisticRegression()\n",
    "        # clf = RandomForestClassifier(max_depth=20)\n",
    "        # clf = GaussianNB()\n",
    "        # clf = LinearDiscriminantAnalysis()\n",
    "        # clf = QuadraticDiscriminantAnalysis()\n",
    "        # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "        # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(10, 10), max_iter=1450)\n",
    "        true_data = stat.datasets['true'][features]\n",
    "        # true_data = lmd.lead_std(stat.datasets['true'][features], take_std_from=stat1.datasets['false_res'][features], axis=1)\n",
    "        # false_data = stat.random_samples()[features]\n",
    "        # false_data = lmd.lead_std(stat.datasets['false_res'][features], take_std_from=stat1.datasets['true'][features], axis=1)\n",
    "        # false_data = stat1.datasets['false_res'][features]\n",
    "        # false_data = stat1.datasets['false'][features]\n",
    "        # true_data = stat1.random_samples()[features]\n",
    "        # false_data = stat.random_samples()[features]\n",
    "        false_data = stat.datasets['false_mirror'][features]\n",
    "        true_data = true_data.assign(resected=True)\n",
    "        false_data = false_data.assign(resected=False)\n",
    "        # samples = [[sample] for sample in true_data.index.tolist() + false_data.index.tolist()]\n",
    "        dataset = pd.concat([true_data, false_data], axis=0)\n",
    "        dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        # scaler = MinMaxScaler()\n",
    "        # scaler = MaxAbsScaler()\n",
    "        # scaler = RobustScaler()\n",
    "\n",
    "\n",
    "\n",
    "        y = dataset['resected'].to_numpy()\n",
    "        dataset = dataset.drop(['resected'], axis=1)\n",
    "        samples = [[sample] for sample in dataset.index.tolist()]\n",
    "        # dataset = lmd.lead_std(dataset, take_std_from=stat1.datasets['false_res'][features], axis=1)\n",
    "        # dataset = lmd.suppress(dataset, axis=1, optimal='max')\n",
    "        # dataset = lmd.promote(dataset, axis=1, optimal='max')\n",
    "        # dataset = lmd.clusterize(dataset, axis=1, n_clusters=3, optimal='symclose')\n",
    "        # dataset = lmd.binarize(dataset, axis=1)\n",
    "        x = scaler.fit_transform(dataset)\n",
    "        x = np.append(x, samples, axis=1)\n",
    "        # x = dataset\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "        train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "        x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        pred = clf.predict(x_test)\n",
    "\n",
    "        for predicted, actual, sample, value in zip(pred, y_test, test_samples, x_test):\n",
    "            nodes_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "            subject_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "            samples_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "\n",
    "\n",
    "        # prob = clf.predict_proba(x_test).tolist()\n",
    "        # for p, x, y in zip(prob, pred, y_test):\n",
    "        #     print(y, x, p)\n",
    "\n",
    "        # df = pd.DataFrame(np.array([np.array(y_test), pred]).T, columns=['actually', 'prediction'])\n",
    "        # print(df)\n",
    "\n",
    "        acc.append(accuracy_score(y_test, pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "        spec.append(tn / (tn + fp))\n",
    "        sens.append(tp / (tp + fn))\n",
    "\n",
    "        if np.isnan(tp/(tp + fp)):\n",
    "            pospred.append(0)\n",
    "        else:\n",
    "            pospred.append(tp/(tp + fp))\n",
    "\n",
    "        if np.isnan((tn/(tn + fn))):\n",
    "            negpred.append(0)\n",
    "        else:\n",
    "            negpred.append(tn/(tn + fn))\n",
    "\n",
    "    cross_nodes_informativeness.informativeness = nodes_informativeness\n",
    "    cross_subjects_informativeness.informativeness = subject_informativeness\n",
    "    cross_samples_informativeness.informativeness = samples_informativeness\n",
    "\n",
    "cross_subjects_informativeness_arr.append(cross_subjects_informativeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  B1C2: 0.12394462546580498,\n",
      "  B1R1: 0.9327164241599497,\n",
      "  G1R1: 0.9517712420605139,\n",
      "  G1V2: 0.4745371591154801,\n",
      "  J1T2: 0.3495300939812037,\n",
      "  K1V1: 0.0793175168329397,\n",
      "  K4L2: 0.4907501321409694,\n",
      "  L1P1: 0.7792342653458461,\n",
      "  L2M1: 0.41611677664467106,\n",
      "  M1G2: 0.6255241600169855,\n",
      "  M1N2: 0.9225707172979296,\n",
      "  M2S2: 0.7351594273629413,\n",
      "  N2K2: 0.39590453168847767,\n",
      "  N3S2: 0.02140227720229432,\n",
      "  O1O2: 0.921661978703252,\n",
      "  P1H2: 0.4975893933306549,\n",
      "  R1D2: 0.9454475944006587,\n",
      "  S1A2: 0.25445323719025936,\n",
      "  S1B1: 0.4881904146803073,\n",
      "  S1H1: 0.4031844850527271,\n",
      "  S1U3: 0.03963650425367363,\n",
      "  S3R1: 0.3317720530835285,\n",
      "}\n",
      "\n",
      "{\n",
      "  B1C2: 0.26068770077349357,\n",
      "  B1R1: 0.9507098229550127,\n",
      "  G1R1: 0.9610440202571094,\n",
      "  G1V2: 0.5000956205775483,\n",
      "  J1T2: 0.7401606598221573,\n",
      "  K1V1: 0.4898949905319332,\n",
      "  K4L2: 0.4782608695652174,\n",
      "  L1P1: 0.9586038387367104,\n",
      "  L2M1: 0.6181251391672233,\n",
      "  M1G2: 0.9040546112986922,\n",
      "  M1N2: 0.919175503694371,\n",
      "  M2S2: 0.8467868338557993,\n",
      "  N2K2: 0.5014884804556045,\n",
      "  N3S2: 0.019394879751745538,\n",
      "  O1O2: 0.9540671454818774,\n",
      "  P1H2: 0.8731589334320751,\n",
      "  R1D2: 0.8053062817011315,\n",
      "  S1A2: 0.36856544460577995,\n",
      "  S1B1: 0.511953952514689,\n",
      "  S1H1: 0.43616177636796194,\n",
      "  S1U3: 0.0529329270221538,\n",
      "  S3R1: 0.5831788766853427,\n",
      "}\n",
      "\n",
      "{\n",
      "  B1C2: 0.42985304273027947,\n",
      "  B1R1: 0.961449498843485,\n",
      "  G1R1: 1.0,\n",
      "  G1V2: 0.49675924649033293,\n",
      "  J1T2: 0.6982535932704833,\n",
      "  K1V1: 0.4808516574705234,\n",
      "  K4L2: 0.30616841105269194,\n",
      "  L1P1: 0.9614346317007327,\n",
      "  L2M1: 0.4732490717199246,\n",
      "  M1G2: 0.8636275082797584,\n",
      "  M1N2: 0.960099591419816,\n",
      "  M2S2: 0.9232388373863585,\n",
      "  N2K2: 0.7835923667125713,\n",
      "  N3S2: 0.0222052067381317,\n",
      "  O1O2: 0.96186117467582,\n",
      "  P1H2: 0.8798659702375087,\n",
      "  R1D2: 0.9580877183494674,\n",
      "  S1A2: 0.3914649557694177,\n",
      "  S1B1: 0.5785057930483419,\n",
      "  S1H1: 0.42059058260175575,\n",
      "  S1U3: 0.019443904335990664,\n",
      "  S3R1: 0.5908661417322835,\n",
      "}\n",
      "\n",
      "{\n",
      "  B1C2: 0.49759074847414064,\n",
      "  B1R1: 0.38135842399294323,\n",
      "  G1R1: 0.623767307500747,\n",
      "  G1V2: 0.6593395563585266,\n",
      "  J1T2: 0.741126930018047,\n",
      "  K1V1: 0.501849691718047,\n",
      "  K4L2: 0.5006449692399285,\n",
      "  L1P1: 0.5925066814583229,\n",
      "  L2M1: 0.5784843031393722,\n",
      "  M1G2: 0.621328827709586,\n",
      "  M1N2: 0.4510288880094975,\n",
      "  M2S2: 0.5229395046691027,\n",
      "  N2K2: 0.6727525302639413,\n",
      "  N3S2: 0.5339829960721921,\n",
      "  O1O2: 0.3887892376681614,\n",
      "  P1H2: 0.4721067970292376,\n",
      "  R1D2: 0.6378495470657739,\n",
      "  S1A2: 0.47659402744148505,\n",
      "  S1B1: 0.43058481292005213,\n",
      "  S1H1: 0.620768390747543,\n",
      "  S1U3: 0.5171928250393909,\n",
      "  S3R1: 0.5231307243963363,\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['transitivity', 's_metric', 'global_efficiency', 'eigencentrality']\n",
    "\n",
    "print(lmd.dict_to_str(\n",
    "    dict(\n",
    "        sorted(\n",
    "            cross_subjects_informativeness_arr[0].acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    "))\n",
    "\n",
    "print(lmd.dict_to_str(\n",
    "    dict(\n",
    "        sorted(\n",
    "            cross_subjects_informativeness_arr[1].acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    "))\n",
    "\n",
    "print(lmd.dict_to_str(\n",
    "    dict(\n",
    "        sorted(\n",
    "            cross_subjects_informativeness_arr[2].acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    "))\n",
    "\n",
    "print(lmd.dict_to_str(\n",
    "    dict(\n",
    "        sorted(\n",
    "            cross_subjects_informativeness_arr[3].acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s1 = pd.Series(\n",
    "    dict(\n",
    "        sorted(\n",
    "          cross_nodes_informativeness.acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "s2 = pd.Series(\n",
    "    dict(\n",
    "        sorted(\n",
    "          cross_nodes_informativeness.ppv().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "s3 = pd.Series(\n",
    "    dict(\n",
    "        sorted(\n",
    "          cross_nodes_informativeness.npv().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "df = pd.DataFrame([s1, s2, s3], index=['acc', 'ppv', 'npv'])\n",
    "df = df.T\n",
    "\n",
    "df.to_csv('/home/user/Documents/Nodes_informativeness.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s1 = pd.Series(\n",
    "    dict(\n",
    "        sorted(\n",
    "          cross_samples_informativeness.acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "s2 = pd.Series(\n",
    "    dict(\n",
    "        sorted(\n",
    "          cross_samples_informativeness.ppv().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "s3 = pd.Series(\n",
    "    dict(\n",
    "        sorted(\n",
    "          cross_samples_informativeness.npv().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "df = pd.DataFrame([s1, s2, s3], index=['acc', 'ppv', 'npv'])\n",
    "df = df.T\n",
    "\n",
    "df.to_csv('/home/user/Documents/Samples_informativeness.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    sorted(\n",
    "        cross_subjects_informativeness.ppv().items(),\n",
    "        key=lambda item: item[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.bar(data.keys(), data.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Subjects-level positive prediction value (for regions)')\n",
    "plt.show()\n",
    "\n",
    "data = dict(\n",
    "    sorted(\n",
    "        cross_samples_informativeness.ppv().items(),\n",
    "        key=lambda item: item[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.bar(data.keys(), data.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Nodes-level positive prediction value, relatively to subject')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "info_correct = {\n",
    "    key: cross_nodes_informativeness.mean()[1]['correct']['false'][key]\n",
    "    for key in cross_nodes_informativeness.mean()[1]['correct']['false']\n",
    "    if key in cross_nodes_informativeness.mean()[1]['wrong']['false']\n",
    "}\n",
    "\n",
    "info_wrong = {\n",
    "    key: cross_nodes_informativeness.mean()[1]['wrong']['false'][key]\n",
    "        # clf = LogisticRegression(class_weight={True: 1, False: .8})\n",
    "        # clf = LogisticRegression()\n",
    "        # clf = RandomFor\n",
    "    for key in cross_nodes_informativeness.mean()[1]['wrong']['false']\n",
    "    if key in cross_nodes_informativeness.mean()[1]['correct']['false']\n",
    "}\n",
    "\n",
    "info_correct = dict(\n",
    "    sorted(\n",
    "        info_correct.items(),\n",
    "        key= lambda item: item[0],\n",
    "        reverse=False\n",
    "    )\n",
    ")\n",
    "\n",
    "info_wrong = dict(\n",
    "    sorted(\n",
    "        info_wrong.items(),\n",
    "        key= lambda item: item[0],\n",
    "        reverse=False\n",
    "    )\n",
    ")\n",
    "\n",
    "tp = np.array(\n",
    "    list(\n",
    "        info_correct.values()\n",
    "    )\n",
    ")\n",
    "\n",
    "tn = np.array(\n",
    "    list(\n",
    "        info_wrong.values()\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.bar(info_correct.keys(), tp/(tn+tp))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Nodes-level false prediction accuracy, relatively to subject')\n",
    "plt.show()\n",
    "\n",
    "# print(\n",
    "#     dict_to_str(info_wrong)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# features = ['0.5-4Hz_wpli', '4-7Hz_wpli', '7-14Hz_wpli', '14-30Hz_wpli', '30-70Hz_wpli']\n",
    "# features = ['14-30Hz_wpli', '4-7Hz_wpli']\n",
    "# features = ['4-7Hz_wpli', '4-7Hz_envelope']\n",
    "# features = ['4-7Hz_wpli', '4-7Hz_envelope']\n",
    "features = ['wpli', 'envelope']\n",
    "# features = ['4-7Hz_wpli']\n",
    "# features = ['envelope']\n",
    "# features = ['4-7Hz_wpli', '4-7Hz_psd']\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "stat = stat1\n",
    "for i in range(1000):\n",
    "    # clf = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=10)\n",
    "    # clf = AdaBoostClassifier(n_estimators=10, random_state=0)\n",
    "    # clf = svm.SVC(kernel='linear')\n",
    "    clf = svm.SVC(class_weight={True: .9, False: 1}, probability=True)\n",
    "    # clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    # clf = LogisticRegression()\n",
    "    # clf = RandomForestClassifier(max_depth=20)\n",
    "    # clf = GaussianNB()\n",
    "    # clf = LinearDiscriminantAnalysis()\n",
    "    # clf = KMeans(n_clusters=2, algorithm='full')\n",
    "    # clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(10, 10), max_iter=1450)\n",
    "\n",
    "    true_data = stat.datasets['true'][features]\n",
    "    # false_data = stat1.random_samples()[features]\n",
    "    # false_data = stat1.datasets['false_res'][features]\n",
    "    # true_data = stat1.random_samples()[features]\n",
    "    # false_data = stat.random_samples()[features]\n",
    "    # false_data = lmd.lead_std(stat.datasets['false_res'][features], take_std_from=stat.datasets['true'][features], axis=1)\n",
    "    false_data = stat.datasets['false_mirror'][features]\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # scaler = MinMaxScaler()\n",
    "    # scaler = MaxAbsScaler()\n",
    "    # scaler = RobustScaler()\n",
    "\n",
    "\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "\n",
    "\n",
    "    # dataset = lmd.lead_std(dataset, take_std_from=stat1.datasets['false_res'][features], axis=1)\n",
    "    dataset = lmd.suppress(dataset, axis=1, optimal='max')\n",
    "    # dataset = lmd.promote(dataset, axis=1, optimal='min')\n",
    "    # dataset = lmd.clusterize(dataset, axis=1, n_clusters=3, optimal='max')\n",
    "    # dataset = lmd.binarize(dataset, axis=1)\n",
    "\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    # x = dataset\n",
    "    scores = cross_val_score(clf, x, y, cv=10)\n",
    "    acc.append(scores.mean())\n",
    "\n",
    "print('Accuracy: ', sum(acc)/len(acc), min(acc), max(acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from nodestimation.mlearning.features import prepare_connectivity, prepare_data\n",
    "# con = pkl.load(open(subjects[0].data['con'], 'rb'))\n",
    "\n",
    "subjects_dir, subjects_ = find_subject_dir()\n",
    "labels = mne.read_labels_from_annot('B1C2', parc='aparc', subjects_dir=subjects_dir)\n",
    "label_names = [label.name for label in labels]\n",
    "lh_labels = [name for name in label_names if name.endswith('lh')]\n",
    "rh_labels = [name for name in label_names if name.endswith('rh')]\n",
    "\n",
    "label_ypos_lh = list()\n",
    "\n",
    "for name in lh_labels:\n",
    "    idx = label_names.index(name)\n",
    "    ypos = np.mean(labels[idx].pos[:, 1])\n",
    "    label_ypos_lh.append(ypos)\n",
    "\n",
    "try:\n",
    "    idx = label_names.index('Brain-Stem')\n",
    "\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    ypos = np.mean(labels[idx].pos[:, 1])\n",
    "    lh_labels.append('Brain-Stem')\n",
    "    label_ypos_lh.append(ypos)\n",
    "\n",
    "\n",
    "lh_labels = [label for (yp, label) in sorted(zip(label_ypos_lh, lh_labels))]\n",
    "\n",
    "rh_labels = [label[:-2] + 'rh' for label in lh_labels\n",
    "             if label != 'Brain-Stem' and label[:-2] + 'rh' in rh_labels]\n",
    "\n",
    "\n",
    "node_colors = [label.color for label in labels]\n",
    "\n",
    "node_order = lh_labels[::-1] + rh_labels\n",
    "\n",
    "node_angles = mne.viz.circular_layout(label_names, node_order, start_pos=90,\n",
    "                              group_boundaries=[0, len(label_names) // 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# methods = [\n",
    "#     'coh',\n",
    "#     'imcoh',\n",
    "#     'plv',\n",
    "#     'ciplv',\n",
    "#     'ppc',\n",
    "#     'pli'\n",
    "# ]\n",
    "#\n",
    "# freq_bands = [\n",
    "#     '0.5-4Hz',\n",
    "#     '4-7Hz',\n",
    "#     '7-14Hz',\n",
    "#     '14-30Hz'\n",
    "# ]\n",
    "#\n",
    "# for method in methods:\n",
    "#     for freq_band in freq_bands:\n",
    "#         fig = plt.figure(num=None, figsize=(25, 25), facecolor='black')\n",
    "#         mne.viz.plot_connectivity_circle(con[freq_band][method]['con'][:, :, 0], label_names, n_lines=300,\n",
    "#                                          node_angles=node_angles, node_colors=node_colors,\n",
    "#                                          title='All-to-All Connectivity Epilepsy Condition ({} for {})'\n",
    "#                                          .format(method, freq_band), padding=8, fontsize_title=35, fontsize_colorbar=25,\n",
    "#                                          fontsize_names=20, fig=fig\n",
    "#                                          )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for subject in [subjects[0]]:\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    display = nplt.plot_glass_brain(None, display_mode='lyrz', figure=fig, axes=ax)\n",
    "    spared = [node.center_coordinates for node in subject.nodes if node.type == 'spared']\n",
    "    resected = [node.center_coordinates for node in subject.nodes if node.type == 'resected']\n",
    "    # resection = read['resec-mni'](subject.data['resec-mni'])\n",
    "    # display.add_markers(resection, marker_color=\"violet\", marker_size=1)\n",
    "    display.add_markers(np.array(spared), marker_color=\"yellow\", marker_size=100)\n",
    "    display.add_markers(np.array(resected), marker_color=\"red\", marker_size=250)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for subject in subjects[0:1]:\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    display = nplt.plot_glass_brain(None, display_mode='lyrz', figure=fig, axes=ax)\n",
    "    spared = [node.center_coordinates for node in subject.nodes if node.type == 'spared']\n",
    "    resected = [node.center_coordinates for node in subject.nodes if node.type == 'resected']\n",
    "    # resection = read['resec-mni'](subject.data['resec-mni'])\n",
    "    # display.add_markers(resection, marker_color=\"violet\", marker_size=1)\n",
    "    display.add_markers(np.array(spared), marker_color=\"yellow\", marker_size=100)\n",
    "    display.add_markers(np.array(resected), marker_color=\"red\", marker_size=250)\n",
    "    plt.show()\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(10,4))\n",
    "    nodes = np.array([node.center_coordinates for node in subject.nodes])\n",
    "    print(subject.datasets.keys())\n",
    "    nplt.plot_markers(subject.datasets['between']['4-8Hz_envelope']*subject.datasets['eigen']['4-8Hz_wpli'],\n",
    "                      nodes, node_size=30, node_cmap='YlOrBr')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for subject in subjects[0:1]:\n",
    "\n",
    "    nodes = np.array([node.center_coordinates for node in subject.nodes])\n",
    "    nodes_lh = np.array([node.center_coordinates for node in subject.nodes if 'lh' in node.label.name])\n",
    "    nodes_rh = np.array([node.center_coordinates for node in subject.nodes if 'rh' in node.label.name])\n",
    "    m = subject.connectomes['4-8Hz']['wpli'].mean().mean()\n",
    "\n",
    "\n",
    "    g = lmd.suppress(\n",
    "            subject.connectomes['4-8Hz']['wpli'],\n",
    "            trigger=m*2,\n",
    "            optimal=0\n",
    "        )\n",
    "    labels = subject.connectomes['4-8Hz']['wpli'].index.to_list()\n",
    "    G = nx.from_numpy_array(\n",
    "            subject.connectomes['4-8Hz']['wpli'].to_numpy()\n",
    "        )\n",
    "    mapping = {node: label for node, label in zip(G, labels)}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    lh, rh = graph_to_hemispheres(\n",
    "        G\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    display = nplt.plot_glass_brain(None, display_mode='lyrz', figure=fig, axes=ax)\n",
    "    display.add_graph(\n",
    "        g,\n",
    "        nodes,\n",
    "    )\n",
    "    # display.add_graph(\n",
    "    #     nx.to_numpy_matrix(lh),\n",
    "    #     nodes_lh\n",
    "    # )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}