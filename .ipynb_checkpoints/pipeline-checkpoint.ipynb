{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mne\n",
    "import nilearn.plotting as nplt\n",
    "import nilearn.image as image\n",
    "import pickle\n",
    "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs, corrmap)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from node_estimate import Node\n",
    "from timewindow import TimeWindow, sliding_window\n",
    "import nibabel as nib\n",
    "from parcellation import freesurf_dict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-519e49b026cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwalk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwalk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.*raw\\.fif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mraw_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.*src.*\\.fif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.*source.*space.*\\.fif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "root= './'\n",
    "\n",
    "tree = os.walk(os.path.join(root, 'Source'))\n",
    "\n",
    "raw_files = []\n",
    "src_files = []\n",
    "inv_files = []\n",
    "bem_files = []\n",
    "fwd_files = []\n",
    "trans_files = []\n",
    "epochs_files = []\n",
    "ave_files = []\n",
    "stc_files = []\n",
    "\n",
    "subjects_found = False\n",
    "\n",
    "for walk in tree:\n",
    "    for file in walk[2]:\n",
    "        if re.search(r'.*raw\\.fif', file):\n",
    "            raw_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*src.*\\.fif', file) or re.search(r'.*source.*space.*\\.fif', file):\n",
    "            src_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*inv.*\\.fif', file) or re.search(r'.*inverse.*\\.fif', file):\n",
    "            inv_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*bem.*\\.fif', file):\n",
    "            bem_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*fwd.*\\.fif', file) or re.search(r'.*forward.*\\.fif', file):\n",
    "            fwd_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*trans.*\\.fif', file):\n",
    "            trans_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*epo.*\\.fif', file):\n",
    "            epochs_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*ave.*\\.fif', file):\n",
    "            ave_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*stc.*\\.fif.*', file):\n",
    "            stc_files.append(os.path.join(walk[0], file))\n",
    "\n",
    "    for subdir in walk[1]:\n",
    "        if subdir == 'Subjects' or subdir == 'subjects' and not subjects_found:\n",
    "            subjects_found = True\n",
    "            subjects_dir = os.path.join(walk[0], subdir)\n",
    "        elif subdir == 'Subjects' or subdir == 'subjects' and subjects_found:\n",
    "            raise OSError(\"There are two subjects directories: {}, {}; Only one must be\".format(\n",
    "                subjects_dir, os.path.join(walk[0], subdir)\n",
    "            ))\n",
    "\n",
    "if not raw_files:\n",
    "    raise OSError(\"No one of raw files are found. Raw file must have extension \\'.fif\\' and ends with \\'raw\\'\")\n",
    "\n",
    "if not subjects_found:\n",
    "    raise OSError(\"Subjects directory not found!\")\n",
    "subjects = os.listdir(subjects_dir)\n",
    "\n",
    "subject_dirs = []\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dirs.append(os.path.join(subjects_dir, subject))\n",
    "\n",
    "raw_file = raw_files[0]\n",
    "bem_file = bem_files[0]\n",
    "src_file = src_files[0]\n",
    "fwd_file = fwd_files[0]\n",
    "trans_file = trans_files[0]\n",
    "\n",
    "res_folder = os.path.join(root, 'Pipeline', subjects[0])\n",
    "\n",
    "res_raw_folder = os.path.join(res_folder, 'Raw')\n",
    "res_bem_folder = os.path.join(res_folder, 'Bem')\n",
    "res_src_folder = os.path.join(res_folder, 'Src')\n",
    "res_fwd_folder = os.path.join(res_folder, 'Fwd')\n",
    "res_events_folder = os.path.join(res_folder, 'Events')\n",
    "res_epochs_folder = os.path.join(res_folder, 'Epochs')\n",
    "res_evoked_folder = os.path.join(res_folder, 'Evoked')\n",
    "res_cov_folder = os.path.join(res_folder, 'Cov')\n",
    "res_inv_folder = os.path.join(res_folder, 'Inv')\n",
    "res_sLORETA_folder = os.path.join(res_folder, 'sLORETA')\n",
    "res_nodes_folder = os.path.join(res_folder, 'NodesEstimate')\n",
    "res_resec_folder = os.path.join(res_folder, 'Resection')\n",
    "\n",
    "res_raw_file = os.path.join(res_raw_folder, 'raw.fif')\n",
    "res_bem_file = os.path.join(res_bem_folder, 'raw_bem.fif')\n",
    "res_src_file = os.path.join(res_src_folder, 'raw_src_ico5.fif')\n",
    "res_fwd_file = os.path.join(res_fwd_folder, 'raw_fwd_ico5.fif')\n",
    "res_events_file = os.path.join(res_events_folder, 'raw_eve.fif')\n",
    "res_epochs_file = os.path.join(res_epochs_folder, 'raw_epo.fif')\n",
    "res_evoked_file = os.path.join(res_evoked_folder, 'raw_ave.fif')\n",
    "res_cov_file = os.path.join(res_cov_folder, 'noise_cov.fif')\n",
    "res_inv_file = os.path.join(res_inv_folder, 'raw_inv.fif')\n",
    "res_sLORETA_file = os.path.join(res_sLORETA_folder, 'sLORETA_raw_ave_inv.pkl')\n",
    "res_nodes_strength_file = os.path.join(res_nodes_folder, 'nodes_strength_auc.dat')\n",
    "res_pearson_nodes_file = os.path.join(res_nodes_folder, 'pearson_nodes.pkl')\n",
    "res_plv_nodes_file = os.path.join(res_nodes_folder, 'plv_nodes.pkl')\n",
    "res_resec_file = os.path.join(res_resec_folder, 'resection.pkl')\n",
    "\n",
    "subject_dir = subject_dirs[0]\n",
    "subject = subjects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conductivity = (0.3,)  # for single layer\n",
    "# conductivity = (0.3, 0.006, 0.3)  # for three layers\n",
    "epochs_tmin, epochs_tmax = -15, 15\n",
    "crop_time = 120\n",
    "snr = 0.5  # use SNR smaller than 1 for raw data\n",
    "lambda2 = 1.0 / snr ** 2\n",
    "method = \"sLORETA\"\n",
    "rfreq = 200\n",
    "nfreq = 50\n",
    "lfreq = 1\n",
    "hfreq = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "\n",
    "    except OSError:\n",
    "        print(\"PIPELINE: creation of the directory %s failed\" % path)\n",
    "\n",
    "    else:\n",
    "        print(\"PIPELINE: successfully created the directory %s \" % path)\n",
    "\n",
    "\n",
    "@sliding_window(size=400, overlap=0.5)\n",
    "def do_nothing(sig):\n",
    "\n",
    "    return sig\n",
    "\n",
    "\n",
    "@sliding_window(400, 0.5)\n",
    "def pearson(signals):\n",
    "\n",
    "    nsigmals, lsignals = signals.shape\n",
    "    out = np.zeros((nsigmals, nsigmals))\n",
    "\n",
    "    for i in range(nsigmals):\n",
    "        for j in range(nsigmals):\n",
    "\n",
    "            if i == j:\n",
    "                out[i, j] = 0\n",
    "                continue\n",
    "\n",
    "            out[i, j] = np.corrcoef(signals[i, :], signals[j, :])[0, 1]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "@sliding_window(400, 0.5)\n",
    "def phase_locking_value(signals):\n",
    "\n",
    "    nsigmals, lsignals = signals.shape\n",
    "    out = np.zeros((nsigmals, nsigmals, lsignals))\n",
    "\n",
    "    for i in range(nsigmals):\n",
    "        for j in range(nsigmals):\n",
    "\n",
    "            sig1_fourier = np.fft.fft(signals[i])\n",
    "            sig2_fourier = np.fft.fft(signals[j])\n",
    "            plv_1_2 = []\n",
    "\n",
    "            for k in range(lsignals):\n",
    "                plv_1_2.append(sig1_fourier[k] * np.conj(sig2_fourier[k])/\n",
    "                           (np.abs(sig1_fourier[k]) * np.abs(sig2_fourier[k])))\n",
    "\n",
    "            out[i,j, :] = plv_1_2\n",
    "\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def mean_across_tw(twlist):\n",
    "\n",
    "    if len(twlist[0].data.shape) == 2:\n",
    "        l, w = twlist[0].data.shape\n",
    "        voxel = voxel_from_tw(twlist)\n",
    "        out = np.zeros((l, w))\n",
    "        for i in range(l):\n",
    "            for j in range(w):\n",
    "                out[i, j] = np.mean(voxel[i, j, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    elif len(twlist[0].data.shape) == 3:\n",
    "        l, w, h = twlist[0].data.shape\n",
    "        voxel = voxel_from_tw(twlist)\n",
    "        out = np.zeros((l, w, h))\n",
    "\n",
    "        for i in range(l):\n",
    "            for j in range(w):\n",
    "                for k in range(h):\n",
    "                    out[i, j, k] = np.mean(voxel[i, j, k, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Can not work with dimension less than two and higher than four')\n",
    "\n",
    "\n",
    "def voxel_from_tw(twlist):\n",
    "\n",
    "    if len(twlist[0].data.shape) == 2:\n",
    "        l, w = twlist[0].data.shape\n",
    "        h = len(twlist)\n",
    "        voxel = np.zeros((l, w, h))\n",
    "\n",
    "        for i in range(h):\n",
    "            voxel[:, :, i] = twlist[i].data\n",
    "\n",
    "        return voxel\n",
    "\n",
    "    elif len(twlist[0].data.shape) == 3:\n",
    "        l, w, h = twlist[0].data.shape\n",
    "        d = len(twlist)\n",
    "        voxel = np.zeros((l, w, h, d))\n",
    "\n",
    "        for i in range(d):\n",
    "            voxel[0:twlist[i].data.shape[0],\n",
    "            0:twlist[i].data.shape[1],\n",
    "            0:twlist[i].data.shape[2], i] = twlist[i].data\n",
    "\n",
    "        return voxel\n",
    "\n",
    "    else: raise ValueError('Can not work with dimension less than two and higher than four')\n",
    "\n",
    "\n",
    "def eigencentrality(matrix):\n",
    "    # only the greatest eigenvalue results in the desired centrality measure [Newman et al]\n",
    "    if len(matrix.shape) == 2:\n",
    "        if matrix.shape[0] != matrix.shape[1]:\n",
    "            raise ValueError('Can not compute centrality for non-square matrix')\n",
    "        out = np.real(sp.linalg.eigvals(matrix))\n",
    "\n",
    "        return out\n",
    "\n",
    "    elif len(matrix.shape) == 3:\n",
    "\n",
    "        if matrix.shape[0] != matrix.shape[1]:\n",
    "            raise ValueError('Matrix shape must be: [n x n x m]')\n",
    "\n",
    "        c = [sp.linalg.eigvals(matrix[:, :, i]) for i in range(matrix.shape[-1])]\n",
    "        out = [np.mean(np.real(np.array(c).T[i])) for i in range(matrix.shape[0])]\n",
    "\n",
    "        return np.array(out)\n",
    "\n",
    "    else: raise ValueError('Can not work with dimension less than two and higher than four')\n",
    "\n",
    "\n",
    "def notchfir(raw, lfreq, nfreq, hfreq):\n",
    "\n",
    "    meg_picks = mne.pick_types(raw.info, meg=True, eeg=False, eog=False)\n",
    "    raw_filtered = raw \\\n",
    "        .load_data() \\\n",
    "        .notch_filter(nfreq, meg_picks) \\\n",
    "        .filter(l_freq=lfreq, h_freq=hfreq)\n",
    "\n",
    "    return raw_filtered\n",
    "\n",
    "\n",
    "def artifacts_clean(raw):\n",
    "\n",
    "    ica = ICA(n_components=15, random_state=97)\n",
    "    ica.fit(raw)\n",
    "    ica.exclude = ica.find_bads_eog(raw)[0] + \\\n",
    "                  ica.find_bads_ecg(raw, method='correlation', threshold=3.0)[0]\n",
    "\n",
    "    ica.apply(raw)\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def first_processing(raw):\n",
    "\n",
    "    raw_cropped = raw.crop(tmax=crop_time)\n",
    "    raw_filtered = notchfir(raw_cropped, lfreq, nfreq, hfreq)\n",
    "    # raw_reconstructed = artifacts_clean(raw_filtered)\n",
    "    raw_out = raw_filtered.pick_types(meg=True, eeg=False)\n",
    "\n",
    "    del raw, raw_filtered\n",
    "\n",
    "    return raw_out\n",
    "\n",
    "\n",
    "def nodes_strength(label_tc, method):\n",
    "\n",
    "    if method == 'pearson':\n",
    "        pearson_matrices = pearson(label_tc)\n",
    "        pears_mean = mean_across_tw(pearson_matrices)\n",
    "        n_strength = np.array([])\n",
    "\n",
    "        for i in range(pears_mean.shape[0]):\n",
    "            n_strength = np.append(n_strength, np.sum(pears_mean[i, :]))\n",
    "\n",
    "        return n_strength, pears_mean\n",
    "\n",
    "    elif method == 'plv':\n",
    "        plv_matrices = phase_locking_value(label_tc)\n",
    "        plv_mean = mean_across_tw(plv_matrices)\n",
    "        centrality = eigencentrality(plv_mean)\n",
    "\n",
    "        return centrality, plv_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./Pipeline'):\n",
    "    mkdir('./Pipeline')\n",
    "\n",
    "if not os.path.exists(res_folder):\n",
    "    mkdir(res_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_raw_file):\n",
    "    raw = mne.io.read_raw_fif(res_raw_file)\n",
    "\n",
    "elif os.path.isfile(raw_file):\n",
    "    raw = mne.io.read_raw_fif(raw_file)\n",
    "    raw = first_processing(raw)\n",
    "    path = res_raw_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    raw.save(res_raw_file)\n",
    "\n",
    "else:\n",
    "    raise OSError('PIPELINE: Raw-file not found')\n",
    "\n",
    "\n",
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_src_file):\n",
    "    src = mne.read_source_spaces(res_src_file)\n",
    "\n",
    "elif os.path.isfile(src_file):\n",
    "    src = mne.read_source_spaces(src_file)\n",
    "    path = res_src_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    src.save(res_src_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Source spaces not found, creating a new one...')\n",
    "    src = mne.setup_source_space(subject, spacing='ico5', add_dist='patch', subjects_dir=subjects_dir)\n",
    "    path = res_src_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    src.save(res_src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_bem_file):\n",
    "    bem = mne.read_bem_solution(res_bem_file)\n",
    "\n",
    "elif os.path.isfile(bem_file):\n",
    "    bem = mne.read_bem_solution(bem_file)\n",
    "    path = res_bem_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_bem_solution(res_bem_file, bem)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: BEM-surface not found, creating a new one...')\n",
    "    model = mne.make_bem_model(subject=subject, ico=4, conductivity=conductivity, subjects_dir=subject_dir)\n",
    "    bem = mne.make_bem_solution(model)\n",
    "    path = res_bem_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_bem_solution(res_bem_file, bem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_fwd_file):\n",
    "    fwd = mne.read_forward_solution(res_fwd_file)\n",
    "\n",
    "elif os.path.isfile(fwd_file):\n",
    "    fwd = mne.read_forward_solution(fwd_file)\n",
    "    path = res_fwd_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_forward_solution(res_fwd_file, fwd)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Forward solution not found, creating a new one...')\n",
    "    fwd = mne.make_forward_solution(res_raw_file, trans=trans_file, src=src, bem=bem, meg=True, eeg=False,\n",
    "                                    mindist=5.0, n_jobs=1, verbose=True)\n",
    "    path = res_fwd_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_forward_solution(res_fwd_file, fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "events = np.array([[\n",
    "        raw.first_samp + raw.time_as_index(i)[0],\n",
    "        0,\n",
    "        1\n",
    "    ] for i in range(1, 59)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_epochs_file):\n",
    "    epochs = mne.read_epochs(res_epochs_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Epochs not found, creating a new one...')\n",
    "    epochs = mne.Epochs(raw, events, tmin=-1, tmax=1,\n",
    "                        preload=True).resample(rfreq, npad='auto')\n",
    "    path = res_epochs_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    epochs.save(res_epochs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_evoked_file):\n",
    "    evoked = mne.read_evokeds(res_evoked_file)\n",
    "else:\n",
    "    print('PIPELINE: Evokeds not found, creating a new one...')\n",
    "    evoked = epochs.average()\n",
    "    path = res_evoked_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_evokeds(res_evoked_file, evoked)\n",
    "\n",
    "    evoked = [evoked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evoked[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_cov_file):\n",
    "    noise_cov = mne.read_cov(res_cov_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Noise covariance not found, creating a new one...')\n",
    "    noise_cov = mne.compute_covariance(epochs.copy().pick_types(meg=True, eeg=False, eog=False), tmin=-1, tmax=0,\n",
    "                                       method='empirical')\n",
    "    path = res_cov_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_cov(res_cov_file, noise_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_inv_file):\n",
    "    inv = mne.minimum_norm.read_inverse_operator(res_inv_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Inverse operator not found, creating a new one...')\n",
    "    inv = mne.minimum_norm.make_inverse_operator(epochs.info, fwd, noise_cov, depth=None, fixed=False)\n",
    "    path = res_inv_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.minimum_norm.write_inverse_operator(res_inv_file, inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.isfile(res_sLORETA_file):\n",
    "    print('Reading sLORETA solution...')\n",
    "    stc = pickle.load(open(res_sLORETA_file, 'rb'))\n",
    "    print('sLORETA has been read')\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: sLORETA not found, creating a new one...')\n",
    "    stc = mne.minimum_norm.apply_inverse_epochs(epochs,\n",
    "                                         inv,\n",
    "                                         lambda2,\n",
    "                                         'sLORETA',\n",
    "                                         pick_ori=None\n",
    "                                         )\n",
    "    path = res_sLORETA_folder\n",
    "\n",
    "    mkdir(path)\n",
    "    pickle.dump(stc, open(res_sLORETA_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_parc = mne.read_labels_from_annot(subject, parc='aparc', subjects_dir=subjects_dir)\n",
    "\n",
    "# label_tc = stc.extract_label_time_course(labels_parc, src=inv['src'], mode='mean_flip')\n",
    "\n",
    "label_ts = mne.extract_label_time_course(stc, labels_parc, src, mode='mean_flip', allow_empty=True, return_generator=True)\n",
    "\n",
    "plt.plot(label_ts[0])\n",
    "plt.show()\n",
    "a\n",
    "\n",
    "labels_aseg = mne.get_volume_labels_from_src(src, subject, subjects_dir)\n",
    "\n",
    "labels = labels_parc + labels_aseg\n",
    "\n",
    "label_names = [label.name for label in labels]\n",
    "\n",
    "lh_labels = [name for name in label_names if name.endswith('lh')]\n",
    "\n",
    "rh_labels = [name for name in label_names if name.endswith('rh')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fmin = 30\n",
    "fmax = 70\n",
    "sfreq = 200\n",
    "\n",
    "con, freqs, times, n_epochs, n_tapers = mne.connectivity.spectral_connectivity(\n",
    "    label_ts, method='plv', mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
    "    fmax=fmax, faverage=True, mt_adaptive=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_ypos_lh = list()\n",
    "\n",
    "for name in lh_labels:\n",
    "    idx = label_names.index(name)\n",
    "    ypos = np.mean(labels[idx].pos[:, 1])\n",
    "    label_ypos_lh.append(ypos)\n",
    "\n",
    "try:\n",
    "    idx = label_names.index('Brain-Stem')\n",
    "\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    ypos = np.mean(labels[idx].pos[:, 1])\n",
    "    lh_labels.append('Brain-Stem')\n",
    "    label_ypos_lh.append(ypos)\n",
    "\n",
    "\n",
    "lh_labels = [label for (yp, label) in sorted(zip(label_ypos_lh, lh_labels))]\n",
    "\n",
    "rh_labels = [label[:-2] + 'rh' for label in lh_labels\n",
    "             if label != 'Brain-Stem' and label[:-2] + 'rh' in rh_labels]\n",
    "\n",
    "\n",
    "node_colors = [label.color for label in labels]\n",
    "\n",
    "node_order = lh_labels[::-1] + rh_labels\n",
    "\n",
    "node_angles = mne.viz.circular_layout(label_names, node_order, start_pos=90,\n",
    "                              group_boundaries=[0, len(label_names) // 2])\n",
    "conmat = con[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 8), facecolor='black')\n",
    "mne.viz.plot_connectivity_circle(conmat, label_names, n_lines=300,\n",
    "                         node_angles=node_angles, node_colors=node_colors,\n",
    "                         title='All-to-All Connectivity Epilepsy '\n",
    "                         'Condition (PLV)', fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fmin = 30.\n",
    "# fmax = 50.\n",
    "# sfreq = 200\n",
    "#\n",
    "# con, freqs, times, n_epochs, n_tapers = mne.connectivity.spectral_connectivity(\n",
    "#     label_ts, method='plv', mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
    "#     fmax=fmax, faverage=True, mt_adaptive=True, n_jobs=1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(num=None, figsize=(8, 8), facecolor='black')\n",
    "# mne.viz.plot_connectivity_circle(conmat, label_names, n_lines=300,\n",
    "#                          node_angles=node_angles, node_colors=node_colors,\n",
    "#                          title='All-to-All Connectivity Epilepsy '\n",
    "#                          'Condition (PLV)', fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nodes strength\n",
    "\n",
    "# plt.plot(n_strength, 'o')\n",
    "# plt.title('Node Strength')\n",
    "# plt.xlabel('node: number')\n",
    "# plt.ylabel('node: strength')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## compute roc curve\n",
    "# resected_nodes = 15\n",
    "#\n",
    "# label_ind = np.zeros(len(n_strength))\n",
    "# label_ind[0:resected_nodes] = True\n",
    "# label_ind[resected_nodes+1:] = False\n",
    "# Drs = roc_auc_score(label_ind, n_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_pearson_nodes_file):\n",
    "    print('Reading nodes...')\n",
    "    nodes = pickle.load(open(res_pearson_nodes_file, 'rb'))\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Pearson\\'s Nodes file not found, create a new one')\n",
    "\n",
    "    if not os.path.exists(res_nodes_folder):\n",
    "        mkdir(res_nodes_folder)\n",
    "\n",
    "    nodes = []\n",
    "    n_strength, pearson_connectome = nodes_strength(label_ts, 'pearson')\n",
    "\n",
    "    for i in range(len(n_strength)):\n",
    "        nodes.append(Node(label_ts[i, :], n_strength[i], labels[i], 'Pearson', pearson_connectome[i, :]))\n",
    "\n",
    "    pickle.dump(nodes, open(res_pearson_nodes_file, 'wb'))\n",
    "\n",
    "coordinates = []\n",
    "n_strength = []\n",
    "for node in nodes:\n",
    "    coordinates.append(node.nilearn_coordinates)\n",
    "    n_strength.append(node.strength)\n",
    "\n",
    "nplt.plot_markers(n_strength, coordinates, node_cmap='black_red_r')\n",
    "nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_plv_nodes_file):\n",
    "    print('Reading nodes...')\n",
    "    nodes = pickle.load(open(res_plv_nodes_file, 'rb'))\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: PLV Nodes file not found, create a new one')\n",
    "\n",
    "    if not os.path.exists(res_nodes_folder):\n",
    "        mkdir(res_nodes_folder)\n",
    "\n",
    "    nodes = []\n",
    "    n_strength, plv_connectome = nodes_strength(label_ts, 'plv')\n",
    "\n",
    "    for i in range(len(n_strength)):\n",
    "        nodes.append(Node(label_ts[i, :], n_strength[i], labels[i], 'PLV', plv_connectome[i, :, :]))\n",
    "\n",
    "    pickle.dump(nodes, open(res_plv_nodes_file, 'wb'))\n",
    "\n",
    "coordinates = []\n",
    "n_strength = []\n",
    "for node in nodes:\n",
    "    coordinates.append(node.nilearn_coordinates)\n",
    "    n_strength.append(node.strength)\n",
    "\n",
    "nplt.plot_markers(n_strength, coordinates, node_cmap='black_red_r')\n",
    "nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## example how to get freesurf_dict\n",
    "\n",
    "# vertexes = [mne.vertex_to_mni(\n",
    "#     label.vertices,\n",
    "#     hemis=0 if label.hemi == 'lh' else 1,\n",
    "#     subject=subject, subjects_dir=subjects_dir\n",
    "# )for label in labels]\n",
    "# freesurf_dict_sample = {l[0].name: np.mean(l[1], axis=0) for l in zip(labels, vertexes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # show one label\n",
    "# nplt.plot_markers(np.zeros(vertexes[0].shape[0]), vertexes[0])\n",
    "# nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # show one node\n",
    "#\n",
    "# nplt.plot_markers(np.array([0, 0]), np.array([\n",
    "#     np.mean(vertexes[0], axis=0),\n",
    "#     np.array([1000, 1000, 1000]) ## plot markers does not work with one node\n",
    "# ]))\n",
    "# nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vertexes = [mne.vertex_to_mni(\n",
    "#     label.vertices,\n",
    "#     hemis=0 if label.hemi == 'lh' else 1,\n",
    "#     subject=subject, subjects_dir=subjects_dir\n",
    "# )for label in labels]\n",
    "#\n",
    "# freesurf_dict_sample = {l[0].name: np.mean(l[1], axis=0) for l in zip(labels, vertexes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(res_resec_file):\n",
    "    print('Reading coordinates...')\n",
    "    resec_coordinates = pickle.load(open(res_resec_file, 'rb'))\n",
    "else:\n",
    "    print('PIPELINE: Resection file not found, create a new one')\n",
    "    if not os.path.exists(res_resec_folder):\n",
    "        mkdir(res_resec_folder)\n",
    "\n",
    "    img = nib.load('Source/Subjects/B1C2/resection/resection.nii')\n",
    "    res = np.array(img.get_data().tolist())\n",
    "    img_coordinates = []\n",
    "    for i in range(res.shape[0]):\n",
    "        for j in range(res.shape[1]):\n",
    "            for k in range(res.shape[2]):\n",
    "                if res[i,j,k] != 0:\n",
    "                    coordinates.append(np.array([i, j, k]))\n",
    "    img_coordinates = np.array(coordinates)\n",
    "    mni_coordinates = []\n",
    "    for coordinate in img_coordinates:\n",
    "        mni_coordinates.append(\n",
    "            np.array(\n",
    "                image.coord_transform(\n",
    "                    coordinate[0],\n",
    "                    coordinate[1],\n",
    "                    coordinate[2],\n",
    "                    img.affine\n",
    "                    )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    resec_coordinates = np.array(mni_coordinates)\n",
    "    pickle.dump(coordinates, open(res_resec_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "display = nplt.plot_glass_brain(\n",
    "    None, display_mode='lyrz', figure=fig, axes=ax)\n",
    "display.add_markers(coordinates, marker_color=\"violet\", marker_size=1)\n",
    "\n",
    "node_coordinates = list()\n",
    "\n",
    "for node in nodes:\n",
    "    node_coordinates.append(node.nilearn_coordinates)\n",
    "    n_strength.append(node.strength)\n",
    "\n",
    "display.add_markers(node_coordinates, marker_color=\"yellow\", marker_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spared = list()\n",
    "resected = list()\n",
    "\n",
    "for node_coordinate in node_coordinates:\n",
    "    for resec_coordinate in resec_coordinates:\n",
    "        diff = node_coordinate - resec_coordinate\n",
    "        dist = np.sqrt(diff[0]**2 + diff[1]**2 + diff[2]**2)\n",
    "        if dist <= 1 and not node_coordinate in np.array(resected):\n",
    "            resected.append(node_coordinate)\n",
    "        else:\n",
    "            spared.append(node_coordinate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "\n",
    "display = nplt.plot_glass_brain(\n",
    "    None, display_mode='lyrz', figure=fig, axes=ax)\n",
    "display.add_markers(coordinates, marker_color=\"violet\", marker_size=1)\n",
    "display.add_markers(np.array(spared), marker_color=\"yellow\", marker_size=100)\n",
    "display.add_markers(np.array(resected), marker_color=\"red\", marker_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del nodes, stc, src, raw, fwd,\\\n",
    "    n_strength, bem, labels, label_ts, inv, noise_cov\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
