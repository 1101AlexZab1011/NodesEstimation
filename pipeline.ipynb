{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mne\n",
    "import nilearn.plotting as nplt\n",
    "import nilearn.image as image\n",
    "import pickle\n",
    "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs, corrmap)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from node_estimate import Node\n",
    "from timewindow import TimeWindow, sliding_window\n",
    "import nibabel as nib\n",
    "from parcellation import freesurf_dict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Source/Subjects/B1C2\n"
     ]
    }
   ],
   "source": [
    "root= './'\n",
    "\n",
    "tree = os.walk(os.path.join(root, 'Source'))\n",
    "\n",
    "raw_files = []\n",
    "src_files = []\n",
    "inv_files = []\n",
    "bem_files = []\n",
    "fwd_files = []\n",
    "trans_files = []\n",
    "epochs_files = []\n",
    "ave_files = []\n",
    "stc_files = []\n",
    "\n",
    "subjects_found = False\n",
    "\n",
    "for walk in tree:\n",
    "    for file in walk[2]:\n",
    "        if re.search(r'.*raw\\.fif', file):\n",
    "            raw_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*src.*\\.fif', file) or re.search(r'.*source.*space.*\\.fif', file):\n",
    "            src_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*inv.*\\.fif', file) or re.search(r'.*inverse.*\\.fif', file):\n",
    "            inv_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*bem.*\\.fif', file):\n",
    "            bem_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*fwd.*\\.fif', file) or re.search(r'.*forward.*\\.fif', file):\n",
    "            fwd_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*trans.*\\.fif', file):\n",
    "            trans_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*epo.*\\.fif', file):\n",
    "            epochs_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*ave.*\\.fif', file):\n",
    "            ave_files.append(os.path.join(walk[0], file))\n",
    "        if re.search(r'.*stc.*\\.fif.*', file):\n",
    "            stc_files.append(os.path.join(walk[0], file))\n",
    "\n",
    "    for subdir in walk[1]:\n",
    "        if subdir == 'Subjects' or subdir == 'subjects' and not subjects_found:\n",
    "            subjects_found = True\n",
    "            subjects_dir = os.path.join(walk[0], subdir)\n",
    "        elif subdir == 'Subjects' or subdir == 'subjects' and subjects_found:\n",
    "            raise OSError(\"There are two subjects directories: {}, {}; Only one must be\".format(\n",
    "                subjects_dir, os.path.join(walk[0], subdir)\n",
    "            ))\n",
    "\n",
    "if not raw_files:\n",
    "    raise OSError(\"No one of raw files are found. Raw file must have extension \\'.fif\\' and ends with \\'raw\\'\")\n",
    "\n",
    "if not subjects_found:\n",
    "    raise OSError(\"Subjects directory not found!\")\n",
    "subjects = os.listdir(subjects_dir)\n",
    "\n",
    "subject_dirs = []\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dirs.append(os.path.join(subjects_dir, subject))\n",
    "\n",
    "raw_file = raw_files[0]\n",
    "bem_file = bem_files[0]\n",
    "src_file = src_files[0]\n",
    "fwd_file = fwd_files[0]\n",
    "trans_file = trans_files[0]\n",
    "\n",
    "res_folder = os.path.join(root, 'Pipeline', subjects[0])\n",
    "\n",
    "res_raw_folder = os.path.join(res_folder, 'Raw')\n",
    "res_bem_folder = os.path.join(res_folder, 'Bem')\n",
    "res_src_folder = os.path.join(res_folder, 'Src')\n",
    "res_fwd_folder = os.path.join(res_folder, 'Fwd')\n",
    "res_events_folder = os.path.join(res_folder, 'Events')\n",
    "res_epochs_folder = os.path.join(res_folder, 'Epochs')\n",
    "res_evoked_folder = os.path.join(res_folder, 'Evoked')\n",
    "res_cov_folder = os.path.join(res_folder, 'Cov')\n",
    "res_inv_folder = os.path.join(res_folder, 'Inv')\n",
    "res_sLORETA_folder = os.path.join(res_folder, 'sLORETA')\n",
    "res_nodes_folder = os.path.join(res_folder, 'NodesEstimate')\n",
    "res_resec_folder = os.path.join(res_folder, 'Resection')\n",
    "\n",
    "res_raw_file = os.path.join(res_raw_folder, 'raw.fif')\n",
    "res_bem_file = os.path.join(res_bem_folder, 'raw_bem.fif')\n",
    "res_src_file = os.path.join(res_src_folder, 'raw_src_ico5.fif')\n",
    "res_fwd_file = os.path.join(res_fwd_folder, 'raw_fwd_ico5.fif')\n",
    "res_events_file = os.path.join(res_events_folder, 'raw_eve.fif')\n",
    "res_epochs_file = os.path.join(res_epochs_folder, 'raw_epo.fif')\n",
    "res_evoked_file = os.path.join(res_evoked_folder, 'raw_ave.fif')\n",
    "res_cov_file = os.path.join(res_cov_folder, 'noise_cov.fif')\n",
    "res_inv_file = os.path.join(res_inv_folder, 'raw_inv.fif')\n",
    "res_sLORETA_file = os.path.join(res_sLORETA_folder, 'sLORETA_raw_ave_inv.fif')\n",
    "res_sLORETA_file_lh = os.path.join(res_sLORETA_folder, 'sLORETA_raw_ave_inv.fif-lh.stc')\n",
    "res_sLORETA_file_rh = os.path.join(res_sLORETA_folder, 'sLORETA_raw_ave_inv.fif-rh.stc')\n",
    "res_nodes_strength_file = os.path.join(res_nodes_folder, 'nodes_strength_auc.dat')\n",
    "res_pearson_nodes_file = os.path.join(res_nodes_folder, 'pearson_nodes.pkl')\n",
    "res_plv_nodes_file = os.path.join(res_nodes_folder, 'plv_nodes.pkl')\n",
    "res_resec_file = os.path.join(res_resec_folder, 'resection.pkl')\n",
    "\n",
    "subject_dir = subject_dirs[0]\n",
    "subject = subjects[0]\n",
    "print(subject_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "conductivity = (0.3,)  # for single layer\n",
    "# conductivity = (0.3, 0.006, 0.3)  # for three layers\n",
    "epochs_tmin, epochs_tmax = -15, 15\n",
    "crop_time = 120\n",
    "snr = 0.5  # use SNR smaller than 1 for raw data\n",
    "lambda2 = 1.0 / snr ** 2\n",
    "method = \"sLORETA\"\n",
    "rfreq = 200\n",
    "nfreq = 50\n",
    "lfreq = 1\n",
    "hfreq = 70"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "\n",
    "    except OSError:\n",
    "        print(\"PIPELINE: creation of the directory %s failed\" % path)\n",
    "\n",
    "    else:\n",
    "        print(\"PIPELINE: successfully created the directory %s \" % path)\n",
    "\n",
    "\n",
    "@sliding_window(size=400, overlap=0.5)\n",
    "def do_nothing(sig):\n",
    "\n",
    "    return sig\n",
    "\n",
    "\n",
    "@sliding_window(400, 0.5)\n",
    "def pearson(signals):\n",
    "\n",
    "    nsigmals, lsignals = signals.shape\n",
    "    out = np.zeros((nsigmals, nsigmals))\n",
    "\n",
    "    for i in range(nsigmals):\n",
    "        for j in range(nsigmals):\n",
    "\n",
    "            if i == j:\n",
    "                out[i, j] = 0\n",
    "                continue\n",
    "\n",
    "            out[i, j] = np.corrcoef(signals[i, :], signals[j, :])[0, 1]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "@sliding_window(400, 0.5)\n",
    "def phase_locking_value(signals):\n",
    "\n",
    "    nsigmals, lsignals = signals.shape\n",
    "    out = np.zeros((nsigmals, nsigmals, lsignals))\n",
    "\n",
    "    for i in range(nsigmals):\n",
    "        for j in range(nsigmals):\n",
    "\n",
    "            sig1_fourier = np.fft.fft(signals[i])\n",
    "            sig2_fourier = np.fft.fft(signals[j])\n",
    "            plv_1_2 = []\n",
    "\n",
    "            for k in range(lsignals):\n",
    "                plv_1_2.append(sig1_fourier[k] * np.conj(sig2_fourier[k])/\n",
    "                           (np.abs(sig1_fourier[k]) * np.abs(sig2_fourier[k])))\n",
    "\n",
    "            out[i,j, :] = plv_1_2\n",
    "\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def mean_across_tw(twlist):\n",
    "\n",
    "    if len(twlist[0].data.shape) == 2:\n",
    "        l, w = twlist[0].data.shape\n",
    "        voxel = voxel_from_tw(twlist)\n",
    "        out = np.zeros((l, w))\n",
    "        for i in range(l):\n",
    "            for j in range(w):\n",
    "                out[i, j] = np.mean(voxel[i, j, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    elif len(twlist[0].data.shape) == 3:\n",
    "        l, w, h = twlist[0].data.shape\n",
    "        voxel = voxel_from_tw(twlist)\n",
    "        out = np.zeros((l, w, h))\n",
    "\n",
    "        for i in range(l):\n",
    "            for j in range(w):\n",
    "                for k in range(h):\n",
    "                    out[i, j, k] = np.mean(voxel[i, j, k, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Can not work with dimension less than two and higher than four')\n",
    "\n",
    "\n",
    "def voxel_from_tw(twlist):\n",
    "\n",
    "    if len(twlist[0].data.shape) == 2:\n",
    "        l, w = twlist[0].data.shape\n",
    "        h = len(twlist)\n",
    "        voxel = np.zeros((l, w, h))\n",
    "\n",
    "        for i in range(h):\n",
    "            voxel[:, :, i] = twlist[i].data\n",
    "\n",
    "        return voxel\n",
    "\n",
    "    elif len(twlist[0].data.shape) == 3:\n",
    "        l, w, h = twlist[0].data.shape\n",
    "        d = len(twlist)\n",
    "        voxel = np.zeros((l, w, h, d))\n",
    "\n",
    "        for i in range(d):\n",
    "            voxel[0:twlist[i].data.shape[0],\n",
    "            0:twlist[i].data.shape[1],\n",
    "            0:twlist[i].data.shape[2], i] = twlist[i].data\n",
    "\n",
    "        return voxel\n",
    "\n",
    "    else: raise ValueError('Can not work with dimension less than two and higher than four')\n",
    "\n",
    "\n",
    "def eigencentrality(matrix):\n",
    "    # only the greatest eigenvalue results in the desired centrality measure [Newman et al]\n",
    "    if len(matrix.shape) == 2:\n",
    "        if matrix.shape[0] != matrix.shape[1]:\n",
    "            raise ValueError('Can not compute centrality for non-square matrix')\n",
    "        out = np.real(sp.linalg.eigvals(matrix))\n",
    "\n",
    "        return out\n",
    "\n",
    "    elif len(matrix.shape) == 3:\n",
    "\n",
    "        if matrix.shape[0] != matrix.shape[1]:\n",
    "            raise ValueError('Matrix shape must be: [n x n x m]')\n",
    "\n",
    "        c = [sp.linalg.eigvals(matrix[:, :, i]) for i in range(matrix.shape[-1])]\n",
    "        out = [np.mean(np.real(np.array(c).T[i])) for i in range(matrix.shape[0])]\n",
    "\n",
    "        return np.array(out)\n",
    "\n",
    "    else: raise ValueError('Can not work with dimension less than two and higher than four')\n",
    "\n",
    "\n",
    "def notchfir(raw, lfreq, nfreq, hfreq):\n",
    "\n",
    "    meg_picks = mne.pick_types(raw.info, meg=True, eeg=False, eog=False)\n",
    "    raw_filtered = raw \\\n",
    "        .load_data() \\\n",
    "        .notch_filter(nfreq, meg_picks) \\\n",
    "        .filter(l_freq=lfreq, h_freq=hfreq)\n",
    "\n",
    "    return raw_filtered\n",
    "\n",
    "\n",
    "def artifacts_clean(raw):\n",
    "\n",
    "    ica = ICA(n_components=15, random_state=97)\n",
    "    ica.fit(raw)\n",
    "    ica.exclude = ica.find_bads_eog(raw)[0] + \\\n",
    "                  ica.find_bads_ecg(raw, method='correlation', threshold=3.0)[0]\n",
    "\n",
    "    ica.apply(raw)\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def first_processing(raw):\n",
    "\n",
    "    raw_cropped = raw.crop(tmax=crop_time)\n",
    "    raw_filtered = notchfir(raw_cropped, lfreq, nfreq, hfreq)\n",
    "    # raw_reconstructed = artifacts_clean(raw_filtered)\n",
    "    raw_out = raw_filtered.pick_types(meg=True, eeg=False)\n",
    "\n",
    "    del raw, raw_filtered\n",
    "\n",
    "    return raw_out\n",
    "\n",
    "\n",
    "def nodes_strength(label_tc, method):\n",
    "\n",
    "    if method == 'pearson':\n",
    "        pearson_matrices = pearson(label_tc)\n",
    "        pears_mean = mean_across_tw(pearson_matrices)\n",
    "        n_strength = np.array([])\n",
    "\n",
    "        for i in range(pears_mean.shape[0]):\n",
    "            n_strength = np.append(n_strength, np.sum(pears_mean[i, :]))\n",
    "\n",
    "        return n_strength, pears_mean\n",
    "\n",
    "    elif method == 'plv':\n",
    "        plv_matrices = phase_locking_value(label_tc)\n",
    "        plv_mean = mean_across_tw(plv_matrices)\n",
    "        centrality = eigencentrality(plv_mean)\n",
    "\n",
    "        return centrality, plv_mean\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if not os.path.exists('./Pipeline'):\n",
    "    mkdir('./Pipeline')\n",
    "\n",
    "if not os.path.exists(res_folder):\n",
    "    mkdir(res_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ./Pipeline/B1C2/Raw/raw.fif...\n",
      "    Range : 39000 ... 159000 =     39.000 ...   159.000 secs\n",
      "Ready.\n",
      "<Info | 24 non-empty values\n",
      " acq_pars: ACQch001 110113 ACQch002 110112 ACQch003 110111 ACQch004 110122 ...\n",
      " acq_stim: 7 800.000000 2000.000000\n",
      " bads: []\n",
      " ch_names: MEG0113, MEG0112, MEG0111, MEG0122, MEG0123, MEG0121, MEG0132, ...\n",
      " chs: 204 GRAD, 102 MAG\n",
      " custom_ref_applied: False\n",
      " description: Vectorview system at moscow\n",
      " dev_head_t: MEG device -> head transform\n",
      " dig: 151 items (3 Cardinal, 4 HPI, 61 EEG, 83 Extra)\n",
      " events: 1 item (list)\n",
      " experimenter: meg\n",
      " file_id: 4 items (dict)\n",
      " highpass: 1.0 Hz\n",
      " hpi_meas: 1 item (list)\n",
      " hpi_results: 1 item (list)\n",
      " hpi_subsystem: 2 items (dict)\n",
      " line_freq: 50.0\n",
      " lowpass: 70.0 Hz\n",
      " meas_date: 2011-10-11 10:36:23 UTC\n",
      " meas_id: 4 items (dict)\n",
      " nchan: 306\n",
      " proc_history: 1 item (list)\n",
      " proj_id: 1 item (ndarray)\n",
      " proj_name: epilepsy\n",
      " projs: []\n",
      " sfreq: 1000.0 Hz\n",
      " subject_info: 3 items (dict)\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_raw_file):\n",
    "    raw = mne.io.read_raw_fif(res_raw_file)\n",
    "\n",
    "elif os.path.isfile(raw_file):\n",
    "    raw = mne.io.read_raw_fif(raw_file)\n",
    "    raw = first_processing(raw)\n",
    "    path = res_raw_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    raw.save(res_raw_file)\n",
    "\n",
    "else:\n",
    "    raise OSError('PIPELINE: Raw-file not found')\n",
    "\n",
    "\n",
    "print(raw.info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-a5d2cc32e07a>:2: RuntimeWarning: This filename (./Pipeline/B1C2/Src/raw_src_ico5.fif) does not conform to MNE naming conventions. All source space files should end with -src.fif, -src.fif.gz, _src.fif, _src.fif.gz, -fwd.fif, -fwd.fif.gz, _fwd.fif, _fwd.fif.gz, -inv.fif, -inv.fif.gz, _inv.fif or _inv.fif.gz\n",
      "  src = mne.read_source_spaces(res_src_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    2 source spaces read\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_src_file):\n",
    "    src = mne.read_source_spaces(res_src_file)\n",
    "\n",
    "elif os.path.isfile(src_file):\n",
    "    src = mne.read_source_spaces(src_file)\n",
    "    path = res_src_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    src.save(res_src_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Source spaces not found, creating a new one...')\n",
    "    src = mne.setup_source_space(subject, spacing='ico5', add_dist='patch', subjects_dir=subjects_dir)\n",
    "    path = res_src_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    src.save(res_src_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading surfaces...\n",
      "\n",
      "Loading the solution matrix...\n",
      "\n",
      "Homogeneous model surface loaded.\n",
      "Loaded linear_collocation BEM solution from ./Pipeline/B1C2/Bem/raw_bem.fif\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_bem_file):\n",
    "    bem = mne.read_bem_solution(res_bem_file)\n",
    "\n",
    "elif os.path.isfile(bem_file):\n",
    "    bem = mne.read_bem_solution(bem_file)\n",
    "    path = res_bem_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_bem_solution(res_bem_file, bem)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: BEM-surface not found, creating a new one...')\n",
    "    model = mne.make_bem_model(subject=subject, ico=4, conductivity=conductivity, subjects_dir=subject_dir)\n",
    "    bem = mne.make_bem_solution(model)\n",
    "    path = res_bem_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_bem_solution(res_bem_file, bem)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading forward solution from ./Pipeline/B1C2/Fwd/raw_fwd_ico5.fif...\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "    Desired named matrix (kind = 3523) not available\n",
      "    Read MEG forward solution (20458 sources, 306 channels, free orientations)\n",
      "    Source spaces transformed to the forward solution coordinate frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-cbb4aca826bd>:2: RuntimeWarning: This filename (./Pipeline/B1C2/Fwd/raw_fwd_ico5.fif) does not conform to MNE naming conventions. All forward files should end with -fwd.fif, -fwd.fif.gz, _fwd.fif or _fwd.fif.gz\n",
      "  fwd = mne.read_forward_solution(res_fwd_file)\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_fwd_file):\n",
    "    fwd = mne.read_forward_solution(res_fwd_file)\n",
    "\n",
    "elif os.path.isfile(fwd_file):\n",
    "    fwd = mne.read_forward_solution(fwd_file)\n",
    "    path = res_fwd_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_forward_solution(res_fwd_file, fwd)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Forward solution not found, creating a new one...')\n",
    "    fwd = mne.make_forward_solution(res_raw_file, trans=trans_file, src=src, bem=bem, meg=True, eeg=False,\n",
    "                                    mindist=5.0, n_jobs=1, verbose=True)\n",
    "    path = res_fwd_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_forward_solution(res_fwd_file, fwd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "events = np.array([[\n",
    "        raw.first_samp + raw.time_as_index(crop_time/2 - 30)[0],\n",
    "        0,\n",
    "        1\n",
    "    ]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# raw.plot(events=events, start=0, duration=120, color='gray', event_color={1: 'r'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Pipeline/B1C2/Epochs/raw_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =  -15000.00 ...   14995.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_epochs_file):\n",
    "    epochs = mne.read_epochs(res_epochs_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Epochs not found, creating a new one...')\n",
    "    epochs = mne.Epochs(raw, events, tmin=epochs_tmin, tmax=epochs_tmax,\n",
    "                        preload=True).resample(rfreq, npad='auto')\n",
    "    path = res_epochs_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    epochs.save(res_epochs_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Pipeline/B1C2/Evoked/raw_ave.fif ...\n",
      "    Found the data of interest:\n",
      "        t =  -15000.00 ...   14995.00 ms (1)\n",
      "        0 CTF compensation matrices available\n",
      "        nave = 1 - aspect type = 100\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "No baseline correction applied\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_evoked_file):\n",
    "    evoked = mne.read_evokeds(res_evoked_file)\n",
    "else:\n",
    "    print('PIPELINE: Evokeds not found, creating a new one...')\n",
    "    evoked = epochs.average()\n",
    "    path = res_evoked_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_evokeds(res_evoked_file, evoked)\n",
    "\n",
    "    evoked = [evoked]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# evoked[0].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    306 x 306 full covariance (kind = 1) found.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_cov_file):\n",
    "    noise_cov = mne.read_cov(res_cov_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Noise covariance not found, creating a new one...')\n",
    "    noise_cov = mne.compute_covariance(epochs.copy().pick_types(meg=True, eeg=False, eog=False), tmin=epochs_tmin, tmax=0,\n",
    "                                       method='empirical')\n",
    "    path = res_cov_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.write_cov(res_cov_file, noise_cov)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inverse operator decomposition from ./Pipeline/B1C2/Inv/raw_inv.fif...\n",
      "    Reading inverse operator info...\n",
      "    [done]\n",
      "    Reading inverse operator decomposition...\n",
      "    [done]\n",
      "    306 x 306 full covariance (kind = 1) found.\n",
      "    Noise covariance matrix read.\n",
      "    61374 x 61374 diagonal covariance (kind = 2) found.\n",
      "    Source covariance matrix read.\n",
      "    61374 x 61374 diagonal covariance (kind = 6) found.\n",
      "    Orientation priors read.\n",
      "    61374 x 61374 diagonal covariance (kind = 5) found.\n",
      "    Depth priors read.\n",
      "    Did not find the desired covariance matrix (kind = 3)\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "    Source spaces transformed to the inverse solution coordinate frame\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(res_inv_file):\n",
    "    inv = mne.minimum_norm.read_inverse_operator(res_inv_file)\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Inverse operator not found, creating a new one...')\n",
    "    inv = mne.minimum_norm.make_inverse_operator(raw.info, fwd, noise_cov)\n",
    "    path = res_inv_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    mne.minimum_norm.write_inverse_operator(res_inv_file, inv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "if os.path.isfile(res_sLORETA_file_lh) and os.path.isfile(res_sLORETA_file_rh):\n",
    "    stc = mne.read_source_estimate(res_sLORETA_file_lh)\n",
    "    stc.__add__(mne.read_source_estimate(res_sLORETA_file_rh))\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: sLORETA not found, creating a new one...')\n",
    "    stc = mne.minimum_norm.apply_inverse(evoked[0], inv, lambda2, 'sLORETA')\n",
    "    path = res_sLORETA_folder\n",
    "\n",
    "    mkdir(path)\n",
    "\n",
    "    stc.save(res_sLORETA_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labels from parcellation...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No such file ./Source/Subjects/B1C2/label/lh.aparc.annot, no candidate parcellations found in directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-739c927db77e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmne\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_labels_from_annot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msubject\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'aparc'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubjects_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msubjects_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mlabel_tc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextract_label_time_course\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minv\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'src'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'mean_flip'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<decorator-gen-280>\u001B[0m in \u001B[0;36mread_labels_from_annot\u001B[0;34m(subject, parc, hemi, surf_name, annot_fname, regexp, subjects_dir, sort, verbose)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/mne/label.py\u001B[0m in \u001B[0;36mread_labels_from_annot\u001B[0;34m(subject, parc, hemi, surf_name, annot_fname, regexp, subjects_dir, sort, verbose)\u001B[0m\n\u001B[1;32m   2060\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhemi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mannot_fname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhemis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2061\u001B[0m         \u001B[0;31m# read annotation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2062\u001B[0;31m         \u001B[0mannot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mctab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_read_annot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2063\u001B[0m         \u001B[0mlabel_rgbas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mctab\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m255.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2064\u001B[0m         \u001B[0mlabel_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mctab\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/mne/label.py\u001B[0m in \u001B[0;36m_read_annot\u001B[0;34m(fname)\u001B[0m\n\u001B[1;32m   1914\u001B[0m         \u001B[0mcands\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_read_annot_cands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdir_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1915\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcands\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1916\u001B[0;31m             raise IOError('No such file %s, no candidate parcellations '\n\u001B[0m\u001B[1;32m   1917\u001B[0m                           'found in directory' % fname)\n\u001B[1;32m   1918\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: No such file ./Source/Subjects/B1C2/label/lh.aparc.annot, no candidate parcellations found in directory"
     ]
    }
   ],
   "source": [
    "labels = mne.read_labels_from_annot(subject, parc='aparc', subjects_dir=subjects_dir)\n",
    "\n",
    "label_tc = stc.extract_label_time_course(labels, src=inv['src'], mode='mean_flip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "f = np.fft.fftfreq(400, 1/200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nodes strength\n",
    "\n",
    "# plt.plot(n_strength, 'o')\n",
    "# plt.title('Node Strength')\n",
    "# plt.xlabel('node: number')\n",
    "# plt.ylabel('node: strength')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## compute roc curve\n",
    "# resected_nodes = 15\n",
    "#\n",
    "# label_ind = np.zeros(len(n_strength))\n",
    "# label_ind[0:resected_nodes] = True\n",
    "# label_ind[resected_nodes+1:] = False\n",
    "# Drs = roc_auc_score(label_ind, n_strength)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile(res_pearson_nodes_file):\n",
    "    print('Reading nodes...')\n",
    "    nodes = pickle.load(open(res_pearson_nodes_file, 'rb'))\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: Pearson\\'s Nodes file not found, create a new one')\n",
    "\n",
    "    if not os.path.exists(res_nodes_folder):\n",
    "        mkdir(res_nodes_folder)\n",
    "\n",
    "    nodes = []\n",
    "    n_strength, pearson_connectome = nodes_strength(label_tc, 'pearson')\n",
    "\n",
    "    for i in range(len(n_strength)):\n",
    "        nodes.append(Node(label_tc[i, :], n_strength[i], labels[i], 'Pearson', pearson_connectome[i, :]))\n",
    "\n",
    "    pickle.dump(nodes, open(res_pearson_nodes_file, 'wb'))\n",
    "\n",
    "coordinates = []\n",
    "n_strength = []\n",
    "for node in nodes:\n",
    "    coordinates.append(node.nilearn_coordinates)\n",
    "    n_strength.append(node.strength)\n",
    "\n",
    "nplt.plot_markers(n_strength, coordinates, node_cmap='black_red_r')\n",
    "nplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile(res_plv_nodes_file):\n",
    "    print('Reading nodes...')\n",
    "    nodes = pickle.load(open(res_plv_nodes_file, 'rb'))\n",
    "\n",
    "else:\n",
    "    print('PIPELINE: PLV Nodes file not found, create a new one')\n",
    "\n",
    "    if not os.path.exists(res_nodes_folder):\n",
    "        mkdir(res_nodes_folder)\n",
    "\n",
    "    nodes = []\n",
    "    n_strength, plv_connectome = nodes_strength(label_tc[0:10], 'plv')\n",
    "\n",
    "    for i in range(len(n_strength)):\n",
    "        nodes.append(Node(label_tc[i, :], n_strength[i], labels[i], 'PLV', plv_connectome[i, :, :]))\n",
    "\n",
    "    pickle.dump(nodes, open(res_plv_nodes_file, 'wb'))\n",
    "\n",
    "coordinates = []\n",
    "n_strength = []\n",
    "for node in nodes:\n",
    "    coordinates.append(node.nilearn_coordinates)\n",
    "    n_strength.append(node.strength)\n",
    "\n",
    "nplt.plot_markers(n_strength, coordinates, node_cmap='black_red_r')\n",
    "nplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## example how to get freesurf_dict\n",
    "# vertexes = [mne.vertex_to_mni(\n",
    "#     label.vertices,\n",
    "#     hemis=0 if label.hemi == 'lh' else 1,\n",
    "#     subject=subject, subjects_dir=subjects_dir\n",
    "# )for label in labels]\n",
    "# freesurf_dict_sample = {l[0].name: np.mean(l[1], axis=0) for l in zip(labels, vertexes)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.plot(f, nodes[0].connections[1].T, 'x')\n",
    "# plt.show()\n",
    "# plt.plot(f, nodes[0].connections[2].T, 'x')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(label_tc[0])\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(np.fft.fftfreq(len(label_tc[0]), 1/200), np.fft.fft(label_tc[0]))\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(label_tc[1])\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(np.fft.fftfreq(len(label_tc[1]), 1/200), np.fft.fft(label_tc[0]))\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # show one label\n",
    "# nplt.plot_markers(np.zeros(vertexes[0].shape[0]), vertexes[0])\n",
    "# nplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # show one node\n",
    "#\n",
    "# nplt.plot_markers(np.array([0, 0]), np.array([\n",
    "#     np.mean(vertexes[0], axis=0),\n",
    "#     np.array([1000, 1000, 1000]) ## plot markers does not work with one node\n",
    "# ]))\n",
    "# nplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vertexes = [mne.vertex_to_mni(\n",
    "#     label.vertices,\n",
    "#     hemis=0 if label.hemi == 'lh' else 1,\n",
    "#     subject=subject, subjects_dir=subjects_dir\n",
    "# )for label in labels]\n",
    "#\n",
    "# freesurf_dict_sample = {l[0].name: np.mean(l[1], axis=0) for l in zip(labels, vertexes)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile(res_resec_file):\n",
    "    print('Reading coordinates...')\n",
    "    coordinates = pickle.load(open(res_resec_file, 'rb')) \n",
    "else:\n",
    "    print('PIPELINE: Resection file not found, create a new one')\n",
    "    if not os.path.exists(res_resec_folder):\n",
    "        mkdir(res_resec_folder)\n",
    "\n",
    "    img = nib.load('Source/Subjects/B1C2/resection/resection.nii')\n",
    "    res = np.array(img.get_data().tolist())\n",
    "    img_coordinates = []\n",
    "    for i in range(res.shape[0]):\n",
    "        for j in range(res.shape[1]):\n",
    "            for k in range(res.shape[2]):\n",
    "                if res[i,j,k] != 0:\n",
    "                    coordinates.append(np.array([i, j, k]))\n",
    "    img_coordinates = np.array(coordinates)\n",
    "    mni_coordinates = []\n",
    "    for coordinate in img_coordinates:\n",
    "        mni_coordinates.append(\n",
    "            np.array(\n",
    "                image.coord_transform(\n",
    "                    coordinate[0],\n",
    "                    coordinate[1],\n",
    "                    coordinate[2],\n",
    "                    img.affine\n",
    "                    )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    coordinates = np.array(mni_coordinates)\n",
    "    pickle.dump(coordinates, open(res_resec_file, 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del nodes, stc, src, raw, fwd,\\\n",
    "    n_strength, bem, labels, label_tc, inv, noise_cov\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}