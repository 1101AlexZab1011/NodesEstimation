{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "from abc import *\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import mne\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors._dist_metrics import DistanceMetric\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nodestimation.learning.estimation import collect_statistic, \\\n",
    "    compute_importance, collect_cross_statistic, make_selection_map, \\\n",
    "    select, separate_datasets, selected_statistic, choose_best, selected_data, make_feature_selection\n",
    "from nodestimation.learning.informativeness import CrossInformativeness, Informativeness, SubjectsInformativeness, \\\n",
    "    NodesInformativeness\n",
    "from nodestimation.learning.networking import sparse_graph, graph_to_hemispheres, hemispheres_division_modularity, \\\n",
    "    metric_for_hemispheres\n",
    "from nodestimation.processing.features import prepare_features\n",
    "from nodestimation.project import find_subject_dir, conditions_unique_code\n",
    "from nodestimation.pipeline import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nibabel\n",
    "import nilearn.plotting as nplt\n",
    "from nodestimation.project.actions import read\n",
    "import nodestimation as nd\n",
    "from nodestimation.learning.modification import append_series, promote\n",
    "import nodestimation.learning.modification as lmd\n",
    "from nodestimation.project.subject import Subject\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from nodestimation.learning.selection import SubjectsStatistic, Wilcoxon, Mannwhitneyu, Test\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.fftpack import fftfreq, irfft, rfft\n",
    "from scipy.fftpack import fftfreq, irfft, rfft"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subjects = pipeline(\n",
    "    methods=['wpli', 'envelope', 'coh', 'imcoh', 'plv', 'ciplv', 'ppc', 'pli', 'pli2_unbiased', 'wpli2_debiased'],\n",
    "    freq_bands=(4, 8),\n",
    "    centrality_metrics=['eigen', 'between', 'degree', 'info']\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stat = SubjectsStatistic(subjects, 'resected', centrality_metric='eigen')\n",
    "print('stat done')\n",
    "\n",
    "dataset_ = metric_for_hemispheres(subjects, nx.algorithms.global_efficiency)\n",
    "print('graph done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets = [dataset_]\n",
    "names = ['global_efficiency']\n",
    "cross_hemispheres_informativeness_arr = list()\n",
    "cross_subjects_informativeness_arr = list()\n",
    "\n",
    "for dataset, name in zip(datasets, names):\n",
    "    cross_hemispheres_informativeness = CrossInformativeness()\n",
    "    cross_subjects_informativeness = CrossInformativeness()\n",
    "\n",
    "    for _ in range(100):\n",
    "        hemispheres_informatoveness = Informativeness()\n",
    "        subjects_informativeness = SubjectsInformativeness()\n",
    "        acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "        for i in range(100):\n",
    "\n",
    "            y = dataset['resected'].to_numpy()\n",
    "            x = dataset[[f'{name}_for_wpli_4-8Hz', f'{name}_for_envelope_4-8Hz']].to_numpy()\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            x = scaler.fit_transform(x)\n",
    "\n",
    "            samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "            x = np.append(x, samples, axis=1)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "            train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "            x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "            clf = svm.SVC()\n",
    "            clf.fit(x_train, y_train)\n",
    "            pred = clf.predict(x_test)\n",
    "\n",
    "            for predicted, actual, sample, value in zip(pred, y_test, test_samples, x_test):\n",
    "                hemispheres_informatoveness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "                subjects_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "\n",
    "        cross_subjects_informativeness.informativeness = subjects_informativeness\n",
    "        cross_hemispheres_informativeness.informativeness = hemispheres_informatoveness\n",
    "\n",
    "    cross_hemispheres_informativeness_arr.append(cross_hemispheres_informativeness)\n",
    "    cross_subjects_informativeness_arr.append(cross_subjects_informativeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cross_nodes_informativeness = CrossInformativeness()\n",
    "cross_subjects_informativeness = CrossInformativeness()\n",
    "cross_samples_informativeness = CrossInformativeness()\n",
    "\n",
    "for _ in range(100):\n",
    "    features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "    acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "    samples_informativeness = Informativeness()\n",
    "    nodes_informativeness = NodesInformativeness()\n",
    "    subject_informativeness = SubjectsInformativeness()\n",
    "\n",
    "    for _ in range(100):\n",
    "        clf = svm.SVC()\n",
    "        true_data = stat.datasets['true'][features]\n",
    "        false_data = stat.datasets['false_mirror'][features]\n",
    "        true_data = true_data.assign(resected=True)\n",
    "        false_data = false_data.assign(resected=False)\n",
    "        dataset = pd.concat([true_data, false_data], axis=0)\n",
    "        dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        y = dataset['resected'].to_numpy()\n",
    "        dataset = dataset.drop(['resected'], axis=1)\n",
    "        samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "        x = scaler.fit_transform(dataset)\n",
    "        x = np.append(x, samples, axis=1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "        train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "        x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        pred = clf.predict(x_test)\n",
    "\n",
    "        for predicted, actual, sample, value in zip(pred, y_test, test_samples, x_test):\n",
    "            nodes_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "            subject_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "            samples_informativeness.informativeness = sample, actual, 'correct' if predicted == actual else 'wrong'\n",
    "\n",
    "    cross_nodes_informativeness.informativeness = nodes_informativeness\n",
    "    cross_subjects_informativeness.informativeness = subject_informativeness\n",
    "    cross_samples_informativeness.informativeness = samples_informativeness\n",
    "\n",
    "cross_subjects_informativeness_arr.append(cross_subjects_informativeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(lmd.dict_to_str(\n",
    "    dict(\n",
    "        sorted(\n",
    "            cross_subjects_informativeness_arr[0].acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    "))\n",
    "\n",
    "print(lmd.dict_to_str(\n",
    "    dict(\n",
    "        sorted(\n",
    "            cross_subjects_informativeness_arr[1].acc().items(),\n",
    "            key=lambda item: item[0]\n",
    "        )\n",
    "    )\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# all, global efficiency\n",
    "\n",
    "dataset = dataset_.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all, global efficiency\n",
      "acc:  0.6381818181818182\n",
      "spec:  0.7075277777777778\n",
      "sens:  0.6130357142857142\n"
     ]
    }
   ],
   "source": [
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    x = dataset[[\n",
    "        'global_efficiency_for_wpli_4-8Hz',\n",
    "        'global_efficiency_for_envelope_4-8Hz'\n",
    "    ]].to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('all, global efficiency')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all, eigencentrality\n",
      "acc:  0.49774999999999997\n",
      "spec:  0.36949743933229656\n",
      "sens:  0.6374408583432721\n"
     ]
    }
   ],
   "source": [
    "# all, eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "accur = cross_subjects_informativeness_arr[1].acc()\n",
    "\n",
    "for _ in range(100):\n",
    "    clf = svm.SVC()\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('all, eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# high, global efficiency\n",
    "\n",
    "dataset = dataset_.copy()\n",
    "\n",
    "acc = cross_subjects_informativeness_arr[0].acc()\n",
    "\n",
    "for sample, i in zip(dataset.index, range(len(dataset.index))):\n",
    "    subject = sample[:4]\n",
    "    if acc[subject] < 0.60 and i%2:\n",
    "        dataset = dataset.drop(index=f'{subject}_lh')\n",
    "        dataset = dataset.drop(index=f'{subject}_rh')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high, global efficiency\n",
      "acc:  0.9166666666666667\n",
      "spec:  0.925\n",
      "sens:  0.9216666666666666\n"
     ]
    }
   ],
   "source": [
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    x = dataset[[\n",
    "        'global_efficiency_for_wpli_4-8Hz',\n",
    "        'global_efficiency_for_envelope_4-8Hz'\n",
    "    ]].to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('high, global efficiency')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-0100a1208f28>:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan(tp/(tp + fp)):\n",
      "<ipython-input-48-0100a1208f28>:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan(tp/(tp + fp)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high, eigencentrality\n",
      "acc:  0.6842857142857143\n",
      "spec:  0.6861666666666667\n",
      "sens:  0.7498333333333334\n"
     ]
    }
   ],
   "source": [
    "# high, eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "accur = cross_subjects_informativeness_arr[1].acc()\n",
    "\n",
    "for _ in range(100):\n",
    "    clf = svm.SVC()\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        subject = sample[:4]\n",
    "        if accur[subject] < 0.60:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        subject = sample[:4]\n",
    "        if accur[subject] < 0.60:\n",
    "            false_data = false_data.drop(index=sample)\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('high, eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "ENGEL1 = [\n",
    "    'B1C2',\n",
    "    'B1R1',\n",
    "    'G1R1',\n",
    "    'G1V2',\n",
    "    'J1T2',\n",
    "    'K1V1',\n",
    "    'L1P1',\n",
    "    'M1G2',\n",
    "    'M1N2',\n",
    "    'O1O2',\n",
    "    'R1D2',\n",
    "    'S1A2',\n",
    "    'S1B1',\n",
    "    'S1H1',\n",
    "    'S1U3'\n",
    "]\n",
    "ENGEL2 = [\n",
    "    'L2M1',\n",
    "    'M2S2',\n",
    "    'N2K2',\n",
    "    'P1H2'\n",
    "]\n",
    "ENGEL34 = [\n",
    "    'N3S2',\n",
    "    'S3R1',\n",
    "    'K4L2'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Engel 1 only, global efficiency\n",
    "\n",
    "dataset = dataset_.copy()\n",
    "\n",
    "acc = cross_subjects_informativeness_arr[0].acc()\n",
    "\n",
    "for sample in dataset.index:\n",
    "    if not any([subject in sample for subject in ENGEL1]):\n",
    "        dataset = dataset.drop(index=sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engel 1 only, global efficiency\n",
      "acc:  0.685\n",
      "spec:  0.7351428571428572\n",
      "sens:  0.673047619047619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-01a775add2e3>:34: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan((tn/(tn + fn))):\n",
      "<ipython-input-51-01a775add2e3>:34: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan((tn/(tn + fn))):\n",
      "<ipython-input-51-01a775add2e3>:29: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan(tp/(tp + fp)):\n",
      "<ipython-input-51-01a775add2e3>:34: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan((tn/(tn + fn))):\n"
     ]
    }
   ],
   "source": [
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    x = dataset[[\n",
    "        'global_efficiency_for_wpli_4-8Hz',\n",
    "        'global_efficiency_for_envelope_4-8Hz'\n",
    "    ]].to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('Engel 1 only, global efficiency')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Engel 1 and 2, global efficiency\n",
    "\n",
    "dataset = dataset_.copy()\n",
    "\n",
    "acc = cross_subjects_informativeness_arr[0].acc()\n",
    "\n",
    "for sample in dataset.index:\n",
    "    if not any([subject in sample for subject in ENGEL1]) and not any([subject in sample for subject in ENGEL2]):\n",
    "        dataset = dataset.drop(index=sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engel 1 and 2, global efficiency\n",
      "acc:  0.7480000000000001\n",
      "spec:  0.8272738095238096\n",
      "sens:  0.7204999999999998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    x = dataset[[\n",
    "        'global_efficiency_for_wpli_4-8Hz',\n",
    "        'global_efficiency_for_envelope_4-8Hz'\n",
    "    ]].to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('Engel 1 and 2, global efficiency')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-bd5a14579f80>:41: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if np.isnan(tp/(tp + fp)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engel 1 only, eigencentrality\n",
      "acc:  0.4276785714285713\n",
      "spec:  0.40640457317594725\n",
      "sens:  0.4969895361022988\n"
     ]
    }
   ],
   "source": [
    "# Engel 1 only, eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "\n",
    "for _ in range(100):\n",
    "    clf = svm.SVC()\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if not any([subject in sample for subject in ENGEL1]):\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if not any([subject in sample for subject in ENGEL1]):\n",
    "            false_data = false_data.drop(index=sample)\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('Engel 1 only, eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engel 1 and 2, eigencentrality\n",
      "acc:  0.48808219178082196\n",
      "spec:  0.3915372690835462\n",
      "sens:  0.6109360917599053\n"
     ]
    }
   ],
   "source": [
    "# Engel 1 and 2, eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "\n",
    "for _ in range(100):\n",
    "    clf = svm.SVC()\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if not any([subject in sample for subject in ENGEL1]) and\\\n",
    "            not any([subject in sample for subject in ENGEL2]):\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if not any([subject in sample for subject in ENGEL1]) and\\\n",
    "                not any([subject in sample for subject in ENGEL2]):\n",
    "            false_data = false_data.drop(index=sample)\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('Engel 1 and 2, eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2a, global efficiency\n",
      "acc:  0.75\n",
      "spec:  0.8636363636363636\n",
      "sens:  0.6363636363636365\n"
     ]
    }
   ],
   "source": [
    "# h2a, global efficiency\n",
    "\n",
    "dataset_high = dataset_.copy()\n",
    "dataset_all = dataset_.copy()\n",
    "\n",
    "acc = cross_subjects_informativeness_arr[0].acc()\n",
    "\n",
    "for sample, i in zip(dataset_high.index, range(len(dataset.index))):\n",
    "    subject = sample[:4]\n",
    "    if acc[subject] < 0.60 and i%2:\n",
    "        dataset_high = dataset_high.drop(index=f'{subject}_lh')\n",
    "        dataset_high = dataset_high.drop(index=f'{subject}_rh')\n",
    "\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    y_train = dataset_high['resected'].to_numpy()\n",
    "    x_train = dataset_high[[\n",
    "        'global_efficiency_for_wpli_4-8Hz',\n",
    "        'global_efficiency_for_envelope_4-8Hz'\n",
    "    ]].to_numpy()\n",
    "    y_test = dataset_all['resected'].to_numpy()\n",
    "    x_test = dataset_all[[\n",
    "        'global_efficiency_for_wpli_4-8Hz',\n",
    "        'global_efficiency_for_envelope_4-8Hz'\n",
    "    ]].to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('h2a, global efficiency')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2a, eigencentrality\n",
      "acc:  0.546875\n",
      "spec:  0.4562499999999999\n",
      "sens:  0.6375\n"
     ]
    }
   ],
   "source": [
    "# h2a, eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "accur = cross_subjects_informativeness_arr[1].acc()\n",
    "\n",
    "for _ in range(1000):\n",
    "    clf = svm.SVC()\n",
    "    true_data_all = stat.datasets['true'][features].copy()\n",
    "    false_data_all = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data_all = true_data_all.assign(resected=True)\n",
    "    false_data_all = false_data_all.assign(resected=False)\n",
    "    true_data_high = true_data_all.copy()\n",
    "    false_data_high = false_data_all.copy()\n",
    "    for sample in true_data_high.index:\n",
    "        subject = sample[:4]\n",
    "        if accur[subject] < 0.60:\n",
    "            true_data_high = true_data_high.drop(index=sample)\n",
    "    for sample in false_data_high.index:\n",
    "        subject = sample[:4]\n",
    "        if accur[subject] < 0.60:\n",
    "            false_data_high = false_data_high.drop(index=sample)\n",
    "    dataset_all = pd.concat([true_data_all, false_data_all], axis=0)\n",
    "    dataset_high = pd.concat([true_data_high, false_data_high], axis=0)\n",
    "    dataset_all = dataset_all.sample(frac = 1)\n",
    "    dataset_high = dataset_high.sample(frac=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y_train = dataset_high['resected'].to_numpy()\n",
    "    dataset_high = dataset_high.drop(['resected'], axis=1)\n",
    "    x_train = scaler.fit_transform(dataset_high)\n",
    "\n",
    "    y_test = dataset_all['resected'].to_numpy()\n",
    "    dataset_all = dataset_all.drop(['resected'], axis=1)\n",
    "    x_test = scaler.fit_transform(dataset_all)\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('h2a, eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high (nodes), eigencentrality\n",
      "acc:  0.9929729729729729\n",
      "spec:  0.995375457875458\n",
      "sens:  0.992388280810361\n"
     ]
    }
   ],
   "source": [
    "# high (nodes), eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "accur = cross_samples_informativeness.acc()\n",
    "\n",
    "for _ in range(100):\n",
    "    clf = svm.SVC()\n",
    "    true_data = stat.datasets['true'][features].copy()\n",
    "    false_data = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data = true_data.assign(resected=True)\n",
    "    false_data = false_data.assign(resected=False)\n",
    "    for sample in true_data.index:\n",
    "        if accur[sample] < 0.60:\n",
    "            true_data = true_data.drop(index=sample)\n",
    "    for sample in false_data.index:\n",
    "        if accur[sample] < 0.60:\n",
    "            false_data = false_data.drop(index=sample)\n",
    "    dataset = pd.concat([true_data, false_data], axis=0)\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y = dataset['resected'].to_numpy()\n",
    "    dataset = dataset.drop(['resected'], axis=1)\n",
    "    samples = [[sample] for sample in dataset.index.tolist()]\n",
    "\n",
    "    x = scaler.fit_transform(dataset)\n",
    "    x = np.append(x, samples, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    train_samples, test_samples = x_train[:, 2], x_test[:, 2]\n",
    "    x_train, x_test = x_train[:, 0:2], x_test[:, 0:2]\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('high (nodes), eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2a, eigencentrality\n",
      "acc:  0.5593750000000002\n",
      "spec:  0.3999999999999999\n",
      "sens:  0.71875\n"
     ]
    }
   ],
   "source": [
    "# h2a, eigencentrality\n",
    "\n",
    "acc, spec, sens, pospred, negpred = list(), list(), list(), list(), list()\n",
    "features = ['4-8Hz_wpli', '4-8Hz_envelope']\n",
    "accur = cross_samples_informativeness.acc()\n",
    "\n",
    "for _ in range(100):\n",
    "    clf = svm.SVC()\n",
    "    true_data_all = stat.datasets['true'][features].copy()\n",
    "    false_data_all = stat.datasets['false_mirror'][features].copy()\n",
    "    true_data_all = true_data_all.assign(resected=True)\n",
    "    false_data_all = false_data_all.assign(resected=False)\n",
    "    true_data_high = true_data_all.copy()\n",
    "    false_data_high = false_data_all.copy()\n",
    "    for sample in true_data_high.index:\n",
    "        if accur[sample] < 0.60:\n",
    "            true_data_high = true_data_high.drop(index=sample)\n",
    "    for sample in false_data_high.index:\n",
    "        if accur[sample] < 0.60:\n",
    "            false_data_high = false_data_high.drop(index=sample)\n",
    "    dataset_all = pd.concat([true_data_all, false_data_all], axis=0)\n",
    "    dataset_high = pd.concat([true_data_high, false_data_high], axis=0)\n",
    "    dataset_all = dataset_all.sample(frac = 1)\n",
    "    dataset_high = dataset_high.sample(frac=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    y_train = dataset_high['resected'].to_numpy()\n",
    "    dataset_high = dataset_high.drop(['resected'], axis=1)\n",
    "    x_train = scaler.fit_transform(dataset_high)\n",
    "\n",
    "    y_test = dataset_all['resected'].to_numpy()\n",
    "    dataset_all = dataset_all.drop(['resected'], axis=1)\n",
    "    x_test = scaler.fit_transform(dataset_all)\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred = clf.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    spec.append(tn / (tn + fp))\n",
    "    sens.append(tp / (tp + fn))\n",
    "\n",
    "    if np.isnan(tp/(tp + fp)):\n",
    "        pospred.append(0)\n",
    "    else:\n",
    "        pospred.append(tp/(tp + fp))\n",
    "\n",
    "    if np.isnan((tn/(tn + fn))):\n",
    "        negpred.append(0)\n",
    "    else:\n",
    "        negpred.append(tn/(tn + fn))\n",
    "\n",
    "print('h2a, eigencentrality')\n",
    "print('acc: ', np.array(acc).mean())\n",
    "print('spec: ', np.array(spec).mean())\n",
    "print('sens: ', np.array(sens).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}